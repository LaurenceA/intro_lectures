{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch as t\n",
    "from torch.distributions import Normal, Categorical, Bernoulli\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib widget\n",
    "from ipywidgets import FloatSlider, IntSlider, interact, interact_manual"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\newcommand{\\bracket}[3]{\\left#1 #3 \\right#2}\n",
    "\\newcommand{\\b}{\\bracket{(}{)}}\n",
    "\\newcommand{\\Bernoulli}{{\\rm Bernoulli}\\b}\n",
    "\\newcommand{\\Categorical}{{\\rm Categorical}\\b}\n",
    "\\newcommand{\\x}{\\mathbf{x}}\n",
    "\\newcommand{\\X}{\\mathbf{X}}\n",
    "\\newcommand{\\m}{\\boldsymbol{\\mu}}\n",
    "\\newcommand{\\P}{{\\rm P}\\b}\n",
    "\\newcommand{\\dd}[2][]{\\frac{\\partial #1}{\\partial #2}}\n",
    "\\newcommand{\\S}{\\mathbf{\\Sigma}}\n",
    "\\newcommand{\\Sh}{\\mathbf{\\hat{\\Sigma}}}\n",
    "\\newcommand{\\mh}{\\boldsymbol{\\hat{\\mu}}}\n",
    "\\newcommand{\\N}{\\mathcal{N}\\b}\n",
    "\\newcommand{\\det}{\\bracket{\\lvert}{\\rvert}}\n",
    "\\newcommand{\\sb}{\\bracket{[}{]}}\n",
    "\\newcommand{\\E}{\\mathbb{E}\\sb}\n",
    "\\newcommand{\\Var}{{\\rm Var}\\sb}\n",
    "\\newcommand{\\Cov}{{\\rm Cov}\\sb}\n",
    "\\DeclareMathOperator*{\\argmax}{arg\\,max}\n",
    "\\DeclareMathOperator*{\\argmin}{arg\\,min}\n",
    "\\newcommand{\\ph}{\\hat{p}}\n",
    "\\newcommand{\\at}{\\bracket{.}{\\rvert}}\n",
    "\\newcommand{\\w}{\\mathbf{w}}\n",
    "\\newcommand{\\W}{\\mathbf{W}}\n",
    "\\newcommand{\\W}{\\mathbf{W}}\n",
    "\\newcommand{\\Wh}{\\mathbf{\\hat{W}}}\n",
    "\\newcommand{\\Y}{\\mathbf{Y}}\n",
    "\\newcommand{\\L}{\\mathcal{L}}\n",
    "\\newcommand{\\wh}{\\mathbf{\\hat{w}}}\n",
    "\\newcommand{\\y}{\\mathbf{y}}\n",
    "\\newcommand{\\0}{\\mathbf{0}}\n",
    "\\newcommand{\\I}{\\mathbf{I}}\n",
    "\\newcommand{\\La}{\\mathbf{\\Lambda}}\n",
    "\\newcommand{\\S}{\\mathbf{\\Sigma}}\n",
    "\\newcommand{\\Sprior}{\\S_\\text{prior}}\n",
    "\\newcommand{\\Spost}{\\S_\\text{post}}\n",
    "\\newcommand{\\mprior}{\\m_\\text{prior}}\n",
    "\\newcommand{\\mpost}{\\m_\\text{post}}\n",
    "\\newcommand{\\Xt}{\\tilde{\\X}}\n",
    "\\newcommand{\\yt}{\\tilde{\\y}}\n",
    "\\newcommand{\\p}{\\mathbf{p}}\n",
    "\\newcommand{\\l}{\\boldsymbol{\\ell}}\n",
    "\\DeclareMathOperator{\\softmax}{softmax}\n",
    "$$\n",
    "\n",
    "<h1> Lecture 4: Classification </h1>\n",
    "\n",
    "Classification is almost exactly the same as regression, except that:\n",
    "<ul>\n",
    "    <li> The outputs, $y$, are discrete class-labels. </li>\n",
    "    <li> Almost all interesting/useful algorithms require iterative solutions </li>\n",
    "</ul>\n",
    "    \n",
    "The same considerations are relevant, including,\n",
    "<ul>\n",
    "    <li> Overfitting </li>\n",
    "    <li> Regularisation </li>\n",
    "    <li> Cross-validation </li>\n",
    "    <li> Bayes (but this is much harder, as there aren't any exact solutions </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Prerequisites: Categorical distribution </h2>\n",
    "\n",
    "Samples from the Categorical distribution are integers $0 \\leq x \\leq K$, with probability given explicity by a length $K$ vector of, $\\p$,\n",
    "\n",
    "\\begin{align}\n",
    "  \\P{y| \\p} &= \\Categorical{y; \\p} = p_x\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([8, 1, 6, 2, 2, 5, 6, 3, 8, 4, 9, 3, 2, 5, 2, 0, 9, 8, 5, 7])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#A uniform Categorical distribution,\n",
    "Py = Categorical(probs=t.ones(10)/10)\n",
    "y = Py.sample((20,))\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000,\n",
       "        0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000,\n",
       "        0.1000, 0.1000])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Py.log_prob(y).exp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 5, 5, 0, 0, 0, 0, 0, 3, 5, 5, 4, 0, 5, 2, 5, 0, 3, 0])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#A non-uniform Categorical distribution,\n",
    "p = t.tensor([0.5, 0.1, 0.1, 0.1, 0.1, 0.1])\n",
    "Py = Categorical(probs=p)\n",
    "y = Py.sample((20,))\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.5000, 0.5000, 0.1000, 0.1000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,\n",
       "        0.1000, 0.1000, 0.1000, 0.1000, 0.5000, 0.1000, 0.1000, 0.1000, 0.5000,\n",
       "        0.1000, 0.5000])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Py.log_prob(y).exp()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Logit parameterisation of the Categorical distribution and the softmax </h3>\n",
    "\n",
    "Working directly with the probabilities turns out to be problematic:\n",
    "<ul>\n",
    "    <li> Probabilities live in a strange range, $0 \\leq p_i \\leq 1$. </li>\n",
    "    <li> Probabilities must sum to $1$. </li>\n",
    "    <li> There is a strong risk of numerical underflow, which breaks algorithms such as (stochastic) gradient descent. </li>\n",
    "</ul>\n",
    "\n",
    "Instead, we can also treat the Categorical parameter as a logits vector, $\\l$, defined such that,\n",
    "\n",
    "\\begin{align}\n",
    "  \\p &= \\softmax(\\l)\\\\\n",
    "  p_i &= \\frac{e^{\\ell_i}}{\\sum_j e^{\\ell_j}}\n",
    "\\end{align}\n",
    "\n",
    "Now, no matter what $\\l$ is, the probabilities must lie in the right range, they must normalize, and they are much less likely to underflow.\n",
    "\n",
    "PyTorch allows you to directly use the logit parameterisation,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([8, 4, 7, 3, 0, 4, 4, 9, 3, 7, 8, 6, 8, 2, 2, 6, 3, 6, 7, 8])"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Uniform Categorical\n",
    "Py = Categorical(logits = t.zeros(10))\n",
    "y = Py.sample((20,))\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000,\n",
       "        0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000,\n",
       "        0.1000, 0.1000])"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Py.log_prob(y).exp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 1, 0, 2, 2, 2, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 3, 0])"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Non-uniform Categorical\n",
    "l = t.tensor([1., 0., -1., -2., -10.])\n",
    "Py = Categorical(logits = l)\n",
    "y = Py.sample((20,))\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.2369, 0.2369, 0.6439, 0.0871, 0.0871, 0.0871, 0.2369, 0.6439, 0.6439,\n",
       "        0.6439, 0.6439, 0.6439, 0.2369, 0.6439, 0.2369, 0.6439, 0.6439, 0.6439,\n",
       "        0.0321, 0.6439])"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Py.log_prob(y).exp()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Formalising maximum-likelihood supervised learning </h2>\n",
    "\n",
    "The most abstract form of a supervised learning problem is given by,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Supervised:\n",
    "    def __init__(self, X_train, Y_train):\n",
    "        self.X_train = X_train\n",
    "        self.Y_train = Y_train\n",
    "        \n",
    "    def L(self, theta):\n",
    "        # use predict to get the distribution P(y| x, W)\n",
    "        Py = self.predict(self.X_train, theta)\n",
    "        # compute the log-probability of the data y under that distribution\n",
    "        return Py.log_prob(self.Y_train).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This class records the training inputs, X, and outputs, Y.\n",
    "\n",
    "The log-likelihood objective, `L`, is a function of the parameters, `theta`.\n",
    "\n",
    "`predict`, takes the training inputs and parameters, and gives a _distribution_ over the training outputs.\n",
    "\n",
    "To compute the objective, we compute the log-probability of the observed training outputs, under the predicted distribution.\n",
    "\n",
    "To get linear regression, we simply need to provide the right `predict` function,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegression(Supervised):\n",
    "    def predict(self, X, theta):\n",
    "        Xb = t.cat([X, t.ones(X.shape[0], 1)], 1)\n",
    "        (W, sigma) = theta\n",
    "        return Normal(Xb@W.T, sigma**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And to get logistic regression, we provide a different `predict` function,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression(Supervised):\n",
    "    def predict(self, X, theta):\n",
    "        Xb = t.cat([X, t.ones(X.shape[0], 1)], 1)\n",
    "        W = theta\n",
    "        return Categorical(logits=Xb@W.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an example, consider the \"Iris\" dataset, which is about classifying flowers based on features such as Petal length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd883c02ce5042b188c50d5546ec0c4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "iris = datasets.load_iris()\n",
    "_X = t.tensor(iris['data'][:, [0, 2]]).float()\n",
    "_Y = t.tensor(iris['target'])\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.set_xlabel(\"$x_{\\lambda 0}$\")\n",
    "ax.set_ylabel(\"$x_{\\lambda 1}$\")\n",
    "ax.scatter(X[:, 0], X[:, 1], c=Y);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the data into train and test (we shuffle the data first, because the classes are ordered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "t.manual_seed(0)\n",
    "perm = t.randperm(X.shape[0])\n",
    "X = _X[perm, :]\n",
    "Y = _Y[perm]\n",
    "\n",
    "X_train = X[:100, :]\n",
    "Y_train = Y[:100]\n",
    "X_test  = X[100:, :]\n",
    "Y_test  = Y[100:]\n",
    "\n",
    "lr = LogisticRegression(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are some more quicker iterative algorithms.  But they don't add much understanding.  So instead, we use PyTorch-magic to do gradient-descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-109.8612289428711\n",
      "-21.01755714416504\n",
      "-17.335208892822266\n",
      "-15.750432968139648\n",
      "-14.77529525756836\n",
      "-14.073020935058594\n",
      "-13.524309158325195\n",
      "-13.075104713439941\n",
      "-12.696579933166504\n",
      "-12.371374130249023\n",
      "-12.088096618652344\n",
      "-11.838754653930664\n",
      "-11.61744499206543\n",
      "-11.419652938842773\n",
      "-11.24185848236084\n",
      "-11.081212043762207\n",
      "-10.935394287109375\n",
      "-10.80250358581543\n",
      "-10.68095588684082\n",
      "-10.56940746307373\n",
      "-10.466708183288574\n",
      "-10.371901512145996\n",
      "-10.284147262573242\n",
      "-10.202718734741211\n",
      "-10.126993179321289\n",
      "-10.056436538696289\n",
      "-9.990544319152832\n",
      "-9.9288911819458\n",
      "-9.87112045288086\n",
      "-9.816884994506836\n",
      "-9.765900611877441\n",
      "-9.717900276184082\n",
      "-9.672636032104492\n",
      "-9.629910469055176\n",
      "-9.589515686035156\n",
      "-9.55129337310791\n",
      "-9.515084266662598\n",
      "-9.480740547180176\n",
      "-9.448126792907715\n",
      "-9.417137145996094\n",
      "-9.387653350830078\n",
      "-9.359598159790039\n",
      "-9.332863807678223\n",
      "-9.307355880737305\n",
      "-9.283014297485352\n",
      "-9.259759902954102\n",
      "-9.237544059753418\n",
      "-9.216300010681152\n",
      "-9.195947647094727\n",
      "-9.176482200622559\n"
     ]
    }
   ],
   "source": [
    "W = t.zeros((3,3), requires_grad=True)\n",
    "\n",
    "for i in range(50000):\n",
    "    L = lr.L(W)\n",
    "    if 0==i % 1000:\n",
    "        print(L.item())\n",
    "    dW = t.autograd.grad(outputs=L, inputs=(W,))[0]\n",
    "    W.data += 0.001*dW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can compute the classification-error,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training correct\n",
      "98/100 = 98.0%\n",
      "Test correct\n",
      "47/50 = 94.0%\n"
     ]
    }
   ],
   "source": [
    "def class_error(X, Y):\n",
    "    Py = lr.predict(X, W)\n",
    "    pred = Py.probs.argmax(1)\n",
    "    N_correct = (pred == Y).sum()\n",
    "    print(f\"{N_correct}/{X.shape[0]} = {100.*N_correct/X.shape[0]}%\")\n",
    "    \n",
    "print(\"Training correct\")\n",
    "class_error(X_train, Y_train)\n",
    "\n",
    "print(\"Test correct\")\n",
    "class_error(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the classifier performs really well on training data, but less well on test data.\n",
    "\n",
    "Indicates evidence of overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
