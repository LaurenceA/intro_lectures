{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch as t\n",
    "from torch.distributions import Normal, Categorical, Bernoulli\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib widget\n",
    "from ipywidgets import FloatSlider, IntSlider, interact, interact_manual"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\newcommand{\\bracket}[3]{\\left#1 #3 \\right#2}\n",
    "\\newcommand{\\b}{\\bracket{(}{)}}\n",
    "\\newcommand{\\Bernoulli}{{\\rm Bernoulli}\\b}\n",
    "\\newcommand{\\Categorical}{{\\rm Categorical}\\b}\n",
    "\\newcommand{\\x}{\\mathbf{x}}\n",
    "\\newcommand{\\X}{\\mathbf{X}}\n",
    "\\newcommand{\\m}{\\boldsymbol{\\mu}}\n",
    "\\newcommand{\\P}{{\\rm P}\\b}\n",
    "\\newcommand{\\dd}[2][]{\\frac{\\partial #1}{\\partial #2}}\n",
    "\\newcommand{\\S}{\\mathbf{\\Sigma}}\n",
    "\\newcommand{\\Sh}{\\mathbf{\\hat{\\Sigma}}}\n",
    "\\newcommand{\\mh}{\\boldsymbol{\\hat{\\mu}}}\n",
    "\\newcommand{\\N}{\\mathcal{N}\\b}\n",
    "\\newcommand{\\det}{\\bracket{\\lvert}{\\rvert}}\n",
    "\\newcommand{\\sb}{\\bracket{[}{]}}\n",
    "\\newcommand{\\E}{\\mathbb{E}\\sb}\n",
    "\\newcommand{\\Var}{{\\rm Var}\\sb}\n",
    "\\newcommand{\\Cov}{{\\rm Cov}\\sb}\n",
    "\\DeclareMathOperator*{\\argmax}{arg\\,max}\n",
    "\\DeclareMathOperator*{\\argmin}{arg\\,min}\n",
    "\\newcommand{\\ph}{\\hat{p}}\n",
    "\\newcommand{\\at}{\\bracket{.}{\\rvert}}\n",
    "\\newcommand{\\w}{\\mathbf{w}}\n",
    "\\newcommand{\\W}{\\mathbf{W}}\n",
    "\\newcommand{\\W}{\\mathbf{W}}\n",
    "\\newcommand{\\Wh}{\\mathbf{\\hat{W}}}\n",
    "\\newcommand{\\Y}{\\mathbf{Y}}\n",
    "\\newcommand{\\L}{\\mathcal{L}}\n",
    "\\newcommand{\\wh}{\\mathbf{\\hat{w}}}\n",
    "\\newcommand{\\y}{\\mathbf{y}}\n",
    "\\newcommand{\\0}{\\mathbf{0}}\n",
    "\\newcommand{\\I}{\\mathbf{I}}\n",
    "\\newcommand{\\La}{\\mathbf{\\Lambda}}\n",
    "\\newcommand{\\S}{\\mathbf{\\Sigma}}\n",
    "\\newcommand{\\Sprior}{\\S_\\text{prior}}\n",
    "\\newcommand{\\Spost}{\\S_\\text{post}}\n",
    "\\newcommand{\\mprior}{\\m_\\text{prior}}\n",
    "\\newcommand{\\mpost}{\\m_\\text{post}}\n",
    "\\newcommand{\\Xt}{\\tilde{\\X}}\n",
    "\\newcommand{\\yt}{\\tilde{\\y}}\n",
    "\\newcommand{\\p}{\\mathbf{p}}\n",
    "\\newcommand{\\l}{\\boldsymbol{\\ell}}\n",
    "\\DeclareMathOperator{\\softmax}{softmax}\n",
    "\\DeclareMathOperator{\\sigmoid}{sigmoid}\n",
    "$$\n",
    "\n",
    "<h1> Lecture 4: Classification </h1>\n",
    "\n",
    "Classification is almost exactly the same as regression, except that:\n",
    "<ul>\n",
    "    <li> The outputs, $y$, are discrete class-labels. </li>\n",
    "    <li> Almost all interesting/useful algorithms require iterative solutions </li>\n",
    "</ul>\n",
    "    \n",
    "The same considerations are relevant, including,\n",
    "<ul>\n",
    "    <li> Overfitting </li>\n",
    "    <li> Regularisation </li>\n",
    "    <li> Cross-validation </li>\n",
    "    <li> Bayes (but this is much harder, as there aren't any exact solutions </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Prerequisites: Bernoulli distribution for two-class classification</h2>\n",
    "Samples from the Bernoulli distribution are either $0$ or $1$, with probability given by the parameter,\n",
    "\n",
    "\\begin{align}\n",
    "  \\P{y| p} &= \\Categorical{y; p} = y p + (1-y) (1-p)\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 1., 1., 1., 0., 1., 0., 1., 1., 1.])\n",
      "tensor([0.8000, 0.8000, 0.8000, 0.8000, 0.2000, 0.8000, 0.2000, 0.8000, 0.8000,\n",
      "        0.8000])\n"
     ]
    }
   ],
   "source": [
    "#A uniform Categorical distribution,\n",
    "Py = Bernoulli(probs=0.8)\n",
    "y = Py.sample((10,))\n",
    "print(y)\n",
    "print(Py.log_prob(y).exp())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Logit parameterisation of the Bernoulli distribution and the sigmoid </h3>\n",
    "\n",
    "Working directly with the probabilities turns out to be problematic:\n",
    "<ul>\n",
    "    <li> Probabilities live in a strange range, $0 \\leq p \\leq 1$. </li>\n",
    "    <li> There is a strong risk of numerical underflow, which breaks algorithms such as (stochastic) gradient descent. </li>\n",
    "</ul>\n",
    "\n",
    "Instead, we can also treat the Categorical parameter as a logits vector, $\\ell$, defined such that,\n",
    "\n",
    "\\begin{align}\n",
    "  p &= \\sigmoid(\\ell) = \\sigma(\\ell)\\\\\n",
    "  p &= \\frac{1}{1+e^{-\\ell}}\n",
    "\\end{align}\n",
    "\n",
    "Now, no matter what $\\l$ is, the probabilities must lie in the right range, and they are much less likely to underflow.\n",
    "\n",
    "PyTorch allows you to directly use the logit parameterisation,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/laurence/programs/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:3: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae5d08c49d71497797233c261e0dbf50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ls = t.linspace(-4, 4, 100)\n",
    "ps = t.sigmoid(ls)\n",
    "fig, ax = plt.subplots()\n",
    "ax.set_xlabel(\"$\\ell$\")\n",
    "ax.set_ylabel(\"$p = \\sigma(\\ell)$\")\n",
    "ax.plot(ls, ps);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Prerequisites: Categorical distribution for multi-class classification </h2>\n",
    "\n",
    "Samples from the Categorical distribution are integers $0 \\leq y < K$, with probability given explicity by a length $K$ vector of, $\\p$,\n",
    "\n",
    "\\begin{align}\n",
    "  \\P{y| \\p} &= \\Categorical{y; \\p} = p_y\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([5, 4, 3, 5, 6, 5, 2, 1, 6, 9, 7, 5, 6, 3, 6, 3, 1, 1, 9, 3])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#A uniform Categorical distribution,\n",
    "Py = Categorical(probs=t.ones(10)/10)\n",
    "y = Py.sample((20,))\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000,\n",
       "        0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000,\n",
       "        0.1000, 0.1000])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Py.log_prob(y).exp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 5, 2, 5, 0, 0, 4, 0, 1, 0, 2, 5, 3, 2, 0, 0, 0, 0, 0, 2])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#A non-uniform Categorical distribution,\n",
    "p = t.tensor([0.5, 0.1, 0.1, 0.1, 0.1, 0.1])\n",
    "Py = Categorical(probs=p)\n",
    "y = Py.sample((20,))\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.5000, 0.1000, 0.1000, 0.1000, 0.5000, 0.5000, 0.1000, 0.5000, 0.1000,\n",
       "        0.5000, 0.1000, 0.1000, 0.1000, 0.1000, 0.5000, 0.5000, 0.5000, 0.5000,\n",
       "        0.5000, 0.1000])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Py.log_prob(y).exp()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Logit parameterisation of the Categorical distribution and the softmax </h3>\n",
    "\n",
    "Working directly with the probabilities turns out to be problematic:\n",
    "<ul>\n",
    "    <li> Probabilities live in a strange range, $0 \\leq p_i \\leq 1$. </li>\n",
    "    <li> Probabilities must sum to $1$. </li>\n",
    "    <li> There is a strong risk of numerical underflow, which breaks algorithms such as (stochastic) gradient descent. </li>\n",
    "</ul>\n",
    "\n",
    "Instead, we can also treat the Categorical parameter as a logits vector, $\\l$, defined such that,\n",
    "\n",
    "\\begin{align}\n",
    "  \\p &= \\softmax(\\l)\\\\\n",
    "  p_i &= \\frac{e^{\\ell_i}}{\\sum_j e^{\\ell_j}}\n",
    "\\end{align}\n",
    "\n",
    "Now, no matter what $\\l$ is, the probabilities must lie in the right range, they must normalize, and they are much less likely to underflow.\n",
    "\n",
    "PyTorch allows you to directly use the logit parameterisation,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 1, 1, 5, 9, 3, 6, 7, 3, 4, 2, 2, 0, 0, 2, 1, 1, 7, 7])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Uniform Categorical\n",
    "Py = Categorical(logits = t.zeros(10))\n",
    "y = Py.sample((20,))\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000,\n",
       "        0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000,\n",
       "        0.1000, 0.1000])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Py.log_prob(y).exp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 3, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Non-uniform Categorical\n",
    "l = t.tensor([1., 0., -1., -2., -10.])\n",
    "Py = Categorical(logits = l)\n",
    "y = Py.sample((20,))\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.2369, 0.0321, 0.6439, 0.6439, 0.6439, 0.2369, 0.6439, 0.6439, 0.2369,\n",
       "        0.6439, 0.6439, 0.6439, 0.6439, 0.6439, 0.6439, 0.6439, 0.6439, 0.6439,\n",
       "        0.6439, 0.6439])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Py.log_prob(y).exp()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Formalising maximum-likelihood supervised learning </h2>\n",
    "\n",
    "The most abstract form of a supervised learning problem is given by,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Supervised:\n",
    "    def __init__(self, X_train, Y_train):\n",
    "        self.X_train = X_train\n",
    "        self.Y_train = Y_train\n",
    "        \n",
    "    def L(self, theta):\n",
    "        # use predict to get the distribution P(y| x, W)\n",
    "        Py = self.predict(self.X_train, theta)\n",
    "        # compute the log-probability of the data y under that distribution\n",
    "        return Py.log_prob(self.Y_train).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This class records the training inputs, X, and outputs, Y.\n",
    "\n",
    "The log-likelihood objective, `L`, is a function of the parameters, `theta`.\n",
    "\n",
    "`predict`, takes the training inputs and parameters, and gives a _distribution_ over the training outputs.\n",
    "\n",
    "To compute the objective, we compute the log-probability of the observed training outputs, under the predicted distribution.\n",
    "\n",
    "To get linear regression, we simply need to provide the right `predict` function,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegression(Supervised):\n",
    "    def predict(self, X, theta):\n",
    "        Xb = t.cat([X, t.ones(X.shape[0], 1)], 1)\n",
    "        (W, sigma) = theta\n",
    "        return Normal(Xb@W.T, sigma**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And to get logistic regression, we provide a different `predict` function,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiLogisticRegression(Supervised):\n",
    "    def predict(self, X, theta):\n",
    "        Xb = t.cat([X, t.ones(X.shape[0], 1)], 1)\n",
    "        W = theta\n",
    "        return Categorical(logits=Xb@W.T)\n",
    "    \n",
    "class TwoLogisticRegression(Supervised):\n",
    "    def predict(self, X, theta):\n",
    "        W = theta\n",
    "        return Bernoulli(logits=X@W.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an example, consider the \"Iris\" dataset, which is about classifying flowers based on features such as Petal length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c50d26e168bc49cd8904f5bb6433f625",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "iris = datasets.load_iris()\n",
    "_X = t.tensor(iris['data'][:, [0, 2]]).float()\n",
    "_Y = t.tensor(iris['target'])\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.set_xlabel(\"$x_{\\lambda 0}$\")\n",
    "ax.set_ylabel(\"$x_{\\lambda 1}$\")\n",
    "ax.scatter(_X[:, 0], _X[:, 1], c=_Y);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the data into train and test (we shuffle the data first, because the classes are ordered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'MultiLogisticRegression' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-86a8402e60ea>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mY_test\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mlr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMultiLogisticRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'MultiLogisticRegression' is not defined"
     ]
    }
   ],
   "source": [
    "t.manual_seed(0)\n",
    "perm = t.randperm(_X.shape[0])\n",
    "X = _X[perm, :]\n",
    "Y = _Y[perm]\n",
    "\n",
    "X_train = X[:100, :]\n",
    "Y_train = Y[:100]\n",
    "X_test  = X[100:, :]\n",
    "Y_test  = Y[100:]\n",
    "\n",
    "lr = MultiLogisticRegression(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are some more quicker iterative algorithms.  But they don't add much understanding.  So instead, we use PyTorch-magic to do gradient-descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-109.8612289428711\n",
      "-21.01755714416504\n",
      "-17.335208892822266\n",
      "-15.750432968139648\n",
      "-14.77529525756836\n",
      "-14.073020935058594\n",
      "-13.524309158325195\n",
      "-13.075104713439941\n",
      "-12.696579933166504\n",
      "-12.371374130249023\n",
      "-12.088096618652344\n",
      "-11.838754653930664\n",
      "-11.61744499206543\n",
      "-11.419652938842773\n",
      "-11.24185848236084\n",
      "-11.081212043762207\n",
      "-10.935394287109375\n",
      "-10.80250358581543\n",
      "-10.68095588684082\n",
      "-10.56940746307373\n",
      "-10.466708183288574\n",
      "-10.371901512145996\n",
      "-10.284147262573242\n",
      "-10.202718734741211\n",
      "-10.126993179321289\n",
      "-10.056436538696289\n",
      "-9.990544319152832\n",
      "-9.9288911819458\n",
      "-9.87112045288086\n",
      "-9.816884994506836\n",
      "-9.765900611877441\n",
      "-9.717900276184082\n",
      "-9.672636032104492\n",
      "-9.629910469055176\n",
      "-9.589515686035156\n",
      "-9.55129337310791\n",
      "-9.515084266662598\n",
      "-9.480740547180176\n",
      "-9.448126792907715\n",
      "-9.417137145996094\n",
      "-9.387653350830078\n",
      "-9.359598159790039\n",
      "-9.332863807678223\n",
      "-9.307355880737305\n",
      "-9.283014297485352\n",
      "-9.259759902954102\n",
      "-9.237544059753418\n",
      "-9.216300010681152\n",
      "-9.195947647094727\n",
      "-9.176482200622559\n"
     ]
    }
   ],
   "source": [
    "W = t.zeros((3,3), requires_grad=True)\n",
    "\n",
    "for i in range(50000):\n",
    "    L = lr.L(W)\n",
    "    if 0==i % 1000:\n",
    "        print(L.item())\n",
    "    dW = t.autograd.grad(outputs=L, inputs=(W,))[0]\n",
    "    W.data += 0.001*dW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can compute the classification-error,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training correct\n",
      "98/100 = 98.0%\n",
      "Test correct\n",
      "47/50 = 94.0%\n"
     ]
    }
   ],
   "source": [
    "def class_error(X, Y):\n",
    "    Py = lr.predict(X, W)\n",
    "    pred = Py.probs.argmax(1)\n",
    "    N_correct = (pred == Y).sum()\n",
    "    print(f\"{N_correct}/{X.shape[0]} = {100.*N_correct/X.shape[0]}%\")\n",
    "    \n",
    "print(\"Training correct\")\n",
    "class_error(X_train, Y_train)\n",
    "\n",
    "print(\"Test correct\")\n",
    "class_error(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the classifier performs really well on training data, but less well on test data.\n",
    "\n",
    "Indicates evidence of overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Overfitting in classification</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c491b88200d49688be710068b58256a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=2, description='order', max=30, min=2), Button(description='Run Interactâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def cheb(xs, c):\n",
    "    # c is int\n",
    "    coefs = c*[0] + [1]\n",
    "    return np.polynomial.chebyshev.chebval(xs, coefs)\n",
    "def chebX(X, order):\n",
    "    assert (-1 <= X).all() and (X <= 1).all()\n",
    "    \n",
    "    xs = []\n",
    "    for c in range(order):\n",
    "        xs.append(cheb(X, c))\n",
    "    return t.cat(xs, 1)\n",
    "t.manual_seed(0)\n",
    "N = 100\n",
    "X = 2*t.rand(N, 1)-1\n",
    "W_true = t.tensor([[4.]])\n",
    "Y = Bernoulli(logits=X@W_true.T).sample()\n",
    "\n",
    "def plot(order):\n",
    "    Xe= chebX(X, order)\n",
    "\n",
    "    lr = TwoLogisticRegression(Xe, Y)\n",
    "\n",
    "    W = t.zeros((1,order), requires_grad=True)\n",
    "\n",
    "\n",
    "    for i in range(15000):\n",
    "        L = lr.L(W)\n",
    "        if 0==i % 2000:\n",
    "            print(L.item())\n",
    "        dW = t.autograd.grad(outputs=L, inputs=(W,))[0]\n",
    "        W.data += 0.001*dW\n",
    "    \n",
    "    fig, ax = plt.subplots()\n",
    "    ax.set_xlabel(\"$x$\")\n",
    "    ax.set_ylabel(\"probability / $y$\")\n",
    "    ax.scatter(X, Y)\n",
    "    xs = t.linspace(-1, 1, 100)[:, None]\n",
    "    ps_fitted = lr.predict(chebX(xs, order), W).probs.detach()\n",
    "    ps_true = lr.predict(xs, W_true).probs.detach()\n",
    "    ax.plot(xs, ps_fitted, label=\"fitted probability\")\n",
    "    ax.plot(xs, ps_true, label=\"true probability\")\n",
    "    ax.legend()\n",
    "    \n",
    "interact_manual(plot, order=IntSlider(min=2, max=30));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use the same techniques to control overfitting:\n",
    "<ul>\n",
    "    <li> Regularisation </li>\n",
    "    <li> Cross-validation (either on the number of classification errors for test points, or on the test-log-likelihood </li>\n",
    "    <li> Bayesian inference (though its much harder here because the posterior over the weights doesn't have an analytiv form)\n",
    "<ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
