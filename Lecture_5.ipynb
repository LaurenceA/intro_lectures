{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch as t\n",
    "from torch.distributions import Normal, Categorical, Bernoulli\n",
    "from torch.distributions import MultivariateNormal as MvNormal\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib widget\n",
    "from ipywidgets import FloatSlider, IntSlider, interact, interact_manual"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\newcommand{\\bracket}[3]{\\left#1 #3 \\right#2}\n",
    "\\newcommand{\\b}{\\bracket{(}{)}}\n",
    "\\newcommand{\\Bernoulli}{{\\rm Bernoulli}\\b}\n",
    "\\newcommand{\\Categorical}{{\\rm Categorical}\\b}\n",
    "\\newcommand{\\x}{\\mathbf{x}}\n",
    "\\newcommand{\\X}{\\mathbf{X}}\n",
    "\\newcommand{\\m}{\\boldsymbol{\\mu}}\n",
    "\\newcommand{\\P}{{\\rm P}\\b}\n",
    "\\newcommand{\\dd}[2][]{\\frac{\\partial #1}{\\partial #2}}\n",
    "\\newcommand{\\S}{\\mathbf{\\Sigma}}\n",
    "\\newcommand{\\Sh}{\\mathbf{\\hat{\\Sigma}}}\n",
    "\\newcommand{\\mh}{\\boldsymbol{\\hat{\\mu}}}\n",
    "\\newcommand{\\N}{\\mathcal{N}\\b}\n",
    "\\newcommand{\\det}{\\bracket{\\lvert}{\\rvert}}\n",
    "\\newcommand{\\sb}{\\bracket{[}{]}}\n",
    "\\newcommand{\\E}{\\mathbb{E}\\sb}\n",
    "\\newcommand{\\Var}{{\\rm Var}\\sb}\n",
    "\\newcommand{\\Cov}{{\\rm Cov}\\sb}\n",
    "\\DeclareMathOperator*{\\argmax}{arg\\,max}\n",
    "\\DeclareMathOperator*{\\argmin}{arg\\,min}\n",
    "\\newcommand{\\ph}{\\hat{p}}\n",
    "\\newcommand{\\at}{\\bracket{.}{\\rvert}}\n",
    "\\newcommand{\\w}{\\mathbf{w}}\n",
    "\\newcommand{\\W}{\\mathbf{W}}\n",
    "\\newcommand{\\W}{\\mathbf{W}}\n",
    "\\newcommand{\\Wh}{\\mathbf{\\hat{W}}}\n",
    "\\newcommand{\\Y}{\\mathbf{Y}}\n",
    "\\newcommand{\\L}{\\mathcal{L}}\n",
    "\\newcommand{\\wh}{\\mathbf{\\hat{w}}}\n",
    "\\newcommand{\\y}{\\mathbf{y}}\n",
    "\\newcommand{\\0}{\\mathbf{0}}\n",
    "\\newcommand{\\I}{\\mathbf{I}}\n",
    "\\newcommand{\\La}{\\mathbf{\\Lambda}}\n",
    "\\newcommand{\\S}{\\mathbf{\\Sigma}}\n",
    "\\newcommand{\\Sprior}{\\S_\\text{prior}}\n",
    "\\newcommand{\\Spost}{\\S_\\text{post}}\n",
    "\\newcommand{\\mprior}{\\m_\\text{prior}}\n",
    "\\newcommand{\\mpost}{\\m_\\text{post}}\n",
    "\\newcommand{\\Xt}{\\tilde{\\X}}\n",
    "\\newcommand{\\yt}{\\tilde{\\y}}\n",
    "\\newcommand{\\p}{\\mathbf{p}}\n",
    "\\newcommand{\\l}{\\boldsymbol{\\ell}}\n",
    "\\DeclareMathOperator*{\\softmax}{softmax}\n",
    "\\DeclareMathOperator*{\\sigmoid}{sigmoid}\n",
    "$$\n",
    "\n",
    "<h1> Lecture 4: Classification </h1>\n",
    "\n",
    "Classification is almost exactly the same as regression, except that:\n",
    "<ul>\n",
    "    <li> The outputs, $y$, are discrete class-labels. </li>\n",
    "    <li> Almost all interesting/useful algorithms require iterative solutions </li>\n",
    "</ul>\n",
    "    \n",
    "The same considerations are relevant, including,\n",
    "<ul>\n",
    "    <li> Overfitting </li>\n",
    "    <li> Regularisation </li>\n",
    "    <li> Cross-validation </li>\n",
    "    <li> Bayes (but this is much harder, as there aren't any exact solutions) </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Prerequisites: Bernoulli distribution for two-class classification</h2>\n",
    "Samples from the Bernoulli distribution are either $0$ or $1$, with probability given by the parameter,\n",
    "\n",
    "\\begin{align}\n",
    "  \\P{y| p} &= \\Bernoulli{y; p} = y p + (1-y) (1-p)\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 1., 0., 0., 1., 1., 0., 1., 0., 0.])\n",
      "tensor([0.2000, 0.8000])\n"
     ]
    }
   ],
   "source": [
    "Py = Bernoulli(probs=0.8)\n",
    "print(Py.sample((10,)))\n",
    "print(Py.log_prob(t.tensor([0., 1.])).exp())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Logit parameterisation of the Bernoulli distribution and the sigmoid </h3>\n",
    "\n",
    "Working directly with the probabilities turns out to be problematic:\n",
    "<ul>\n",
    "    <li> Probabilities live in a strange range, $0 \\leq p \\leq 1$. </li>\n",
    "    <li> There is a strong risk of numerical underflow, which breaks algorithms such as (stochastic) gradient descent. </li>\n",
    "</ul>\n",
    "\n",
    "Instead, we can also treat the Bernoulli parameter as a logits vector, $\\ell$, defined such that,\n",
    "\n",
    "\\begin{align}\n",
    "  p &= \\sigmoid(\\ell) = \\sigma(\\ell)\\\\\n",
    "  p &= \\frac{1}{1+e^{-\\ell}}\n",
    "\\end{align}\n",
    "\n",
    "Now, no matter what $\\l$ is, the probabilities must lie in the right range, and they are much less likely to underflow.\n",
    "\n",
    "PyTorch allows you to directly use the logit parameterisation,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff6b8639e8a740c8895b0646776c2817",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ls = t.linspace(-4, 4, 100)\n",
    "ps = t.sigmoid(ls)\n",
    "fig, ax = plt.subplots()\n",
    "ax.set_xlabel(\"$\\ell$\")\n",
    "ax.set_ylabel(\"$p = \\sigma(\\ell)$\")\n",
    "ax.plot(ls, ps);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "tensor([0.8808, 0.1192])\n"
     ]
    }
   ],
   "source": [
    "Py = Bernoulli(logits=-2)\n",
    "print(Py.sample((10,)))\n",
    "print(Py.log_prob(t.tensor([0., 1.])).exp())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Prerequisites: Categorical distribution for multi-class classification </h2>\n",
    "\n",
    "The Bernoulli distribution takes one parameter and gives the probability of two classes.  But what about multiple classes?\n",
    "\n",
    "Samples from the Categorical distribution are integers $0 \\leq y < K$, with probability given explicity by a length $K$ vector of, $\\p$,\n",
    "\n",
    "\\begin{align}\n",
    "  \\P{y| \\p} &= \\Categorical{y; \\p} = p_y\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 2, 7, 1, 4, 2, 8, 4, 5, 0, 9, 8, 3, 1, 5, 9, 4, 1, 3, 9])\n"
     ]
    }
   ],
   "source": [
    "#A uniform Categorical distribution,\n",
    "Py = Categorical(probs=t.ones(10)/10)\n",
    "print(Py.sample((20,)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000,\n",
       "        0.1000])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ys = t.arange(10)\n",
    "print(ys)\n",
    "Py.log_prob(ys).exp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 5, 2, 2, 3, 4, 5, 1, 1, 1, 4, 0, 2, 5, 1, 4, 1, 5, 3])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#A non-uniform Categorical distribution,\n",
    "p = t.tensor([0.5, 0.1, 0.1, 0.1, 0.1, 0.1])\n",
    "Py = Categorical(probs=p)\n",
    "y = Py.sample((20,))\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.5000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Py.log_prob(t.arange(6)).exp()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Logit parameterisation of the Categorical distribution and the softmax </h3>\n",
    "\n",
    "Working directly with the probabilities turns out to be problematic:\n",
    "<ul>\n",
    "    <li> Probabilities live in a strange range, $0 \\leq p_i \\leq 1$. </li>\n",
    "    <li> Probabilities must sum to $1$. </li>\n",
    "    <li> There is a strong risk of numerical underflow, which breaks algorithms such as (stochastic) gradient descent. </li>\n",
    "</ul>\n",
    "\n",
    "Instead, we can also treat the Categorical parameter as a logits vector, $\\l$, defined such that,\n",
    "\n",
    "\\begin{align}\n",
    "  \\p &= \\softmax(\\l)\\\\\n",
    "  p_i &= \\frac{e^{\\ell_i}}{\\sum_{j=0}^{K-1} e^{\\ell_j}}\n",
    "\\end{align}\n",
    "\n",
    "Now, no matter what $\\l$ is, the probabilities must lie in the right range, they must normalize, and they are much less likely to underflow.\n",
    "\n",
    "PyTorch allows you to directly use the logit parameterisation,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([4, 8, 9, 2, 2, 9, 2, 2, 3, 7, 5, 9, 1, 7, 6, 0, 9, 6, 7, 7])\n",
      "tensor([0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000,\n",
      "        0.1000])\n"
     ]
    }
   ],
   "source": [
    "# Uniform Categorical\n",
    "Py = Categorical(logits = t.ones(10))\n",
    "print(Py.sample((20,)))\n",
    "print(Py.log_prob(t.arange(10)).exp())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0])\n",
      "tensor([6.4391e-01, 2.3688e-01, 8.7143e-02, 3.2058e-02, 1.0754e-05])\n"
     ]
    }
   ],
   "source": [
    "# Non-uniform Categorical\n",
    "l = t.tensor([1., 0., -1., -2., -10.])\n",
    "Py = Categorical(logits = l)\n",
    "print(Py.sample((20,)))\n",
    "print(Py.log_prob(t.arange(5)).exp())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Formalising maximum-likelihood supervised learning </h2>\n",
    "\n",
    "For linear regression, the goal was to predict the distribution over a floating-point $y_\\lambda$ based on a given value for $\\x_\\lambda$,\n",
    "\n",
    "\\begin{align}\n",
    "  \\P{y_\\lambda| \\x_\\lambda} &= \\N{y_\\lambda; \\x_\\lambda \\cdot \\w, \\sigma^2}\n",
    "\\end{align}\n",
    "\n",
    "In classification, the goal is similar: we again want to predict a distribution over $y_\\lambda$, based on an input, $\\x_\\lambda$.  The only difference is that here, $y_\\lambda$ is an integer from $0$ to $K-1$.  Thus, when we predict a probability over $y_\\lambda$ it either needs to be Bernoulli (for two classes), or categorical (if there can be more classes,\n",
    "\n",
    "\\begin{align}\n",
    "  \\P{y_\\lambda| \\x_\\lambda} &= \\Bernoulli{y_\\lambda; \\sigma\\b{\\x_\\lambda \\cdot \\w}}\\\\\n",
    "  \\P{y_\\lambda| \\x_\\lambda} &= \\Categorical{y_\\lambda; \\softmax\\b{\\x_\\lambda \\W}}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an example, consider the \"Iris\" dataset, which is about classifying flowers based on features such as Petal length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1bbcd077a9ec4c4ab996aadb33f648a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "iris = datasets.load_iris()\n",
    "_X = t.tensor(iris['data'][50:, [0, 2]]).float()\n",
    "_Y = (t.tensor(iris['target'][50:])-1).float()\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.set_xlabel(\"$x_{\\lambda 0}$\")\n",
    "ax.set_ylabel(\"$x_{\\lambda 1}$\")\n",
    "ax.scatter(_X[:, 0], _X[:, 1], c=_Y);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the data into train and test (we shuffle the data first, because the classes are ordered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "t.manual_seed(0)\n",
    "perm = t.randperm(_X.shape[0])\n",
    "X = _X[perm, :]\n",
    "X = t.cat([X, t.ones(X.shape[0], 1)], -1)\n",
    "Y = _Y[perm][:, None]\n",
    "\n",
    "X_train = X[:70, :]\n",
    "Y_train = Y[:70, :]\n",
    "X_test  = X[70:, :]\n",
    "Y_test  = Y[70:, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are some more quicker iterative algorithms.  But they don't add much understanding.  So instead, we use PyTorch-magic to do gradient-descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-48.974510192871094\n",
      "-20.449899673461914\n",
      "-17.061647415161133\n",
      "-15.644773483276367\n",
      "-14.787422180175781\n",
      "-14.168648719787598\n",
      "-13.676431655883789\n",
      "-13.261651992797852\n",
      "-12.899398803710938\n",
      "-12.575631141662598\n",
      "-12.281757354736328\n",
      "-12.01214599609375\n",
      "-11.762899398803711\n",
      "-11.53117561340332\n",
      "-11.31480884552002\n",
      "-11.112078666687012\n",
      "-10.92162799835205\n",
      "-10.742270469665527\n",
      "-10.573012351989746\n",
      "-10.413020133972168\n",
      "-10.261512756347656\n",
      "-10.117838859558105\n",
      "-9.981404304504395\n",
      "-9.851656913757324\n",
      "-9.728116989135742\n",
      "-9.610358238220215\n",
      "-9.49797534942627\n",
      "-9.39061450958252\n",
      "-9.287946701049805\n",
      "-9.189657211303711\n",
      "-9.09546184539795\n",
      "-9.005102157592773\n",
      "-8.918354988098145\n",
      "-8.835001945495605\n",
      "-8.754850387573242\n",
      "-8.677701950073242\n",
      "-8.603403091430664\n",
      "-8.53178882598877\n",
      "-8.46267032623291\n",
      "-8.395984649658203\n",
      "-8.331559181213379\n",
      "-8.269283294677734\n",
      "-8.20904541015625\n",
      "-8.150764465332031\n",
      "-8.094314575195312\n",
      "-8.039603233337402\n",
      "-7.9865803718566895\n",
      "-7.935168266296387\n",
      "-7.88528299331665\n",
      "-7.836832046508789\n"
     ]
    }
   ],
   "source": [
    "W = t.randn((3,1), requires_grad=True)/100\n",
    "\n",
    "for i in range(50000):\n",
    "    L = Bernoulli(logits=X_train@W).log_prob(Y_train).sum()\n",
    "    \n",
    "    dW = t.autograd.grad(outputs=L, inputs=(W,))[0]\n",
    "    if 0==i % 1000:\n",
    "        print(L.item())\n",
    "    W.data += 0.001*dW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can plot the classification boundary,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "740f9e05f76545818a4cc4d5227755a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.set_xlabel(\"$x_{\\lambda 0}$\")\n",
    "ax.set_ylabel(\"$x_{\\lambda 1}$\")\n",
    "ax.scatter(_X[:, 0], _X[:, 1], c=_Y);\n",
    "\n",
    "x0range = t.linspace(*ax.get_xlim(), 500)\n",
    "x1range = t.linspace(*ax.get_ylim(), 500)\n",
    "\n",
    "x0s, x1s = t.meshgrid(x0range, x1range)\n",
    "xs = t.stack([x0s, x1s, t.ones(500, 500)], -1)\n",
    "\n",
    "ps = Bernoulli(logits=xs@W.detach()).probs[:, :, 0]\n",
    "ax.contour(x0s, x1s, 0.5<ps);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can compute the classification-error,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training correct\n",
      "67/70 = 95.71428680419922%\n",
      "Test correct\n",
      "28/30 = 93.33333587646484%\n"
     ]
    }
   ],
   "source": [
    "def class_error(X, Y):\n",
    "    Py = Bernoulli(logits=X@W)\n",
    "    pred = 0.5 < Py.probs\n",
    "    N_correct = (pred == Y).sum()\n",
    "    print(f\"{N_correct}/{X.shape[0]} = {100.*N_correct/X.shape[0]}%\")\n",
    "    \n",
    "print(\"Training correct\")\n",
    "class_error(X_train, Y_train)\n",
    "\n",
    "print(\"Test correct\")\n",
    "class_error(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the classifier performs really well on training data, but less well on test data.\n",
    "\n",
    "Indicates evidence of overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Overfitting in classification</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b070199c408846cab946d6f8557d4c4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=2, description='order', max=30, min=2), Button(description='Run Interact…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def cheb(xs, c):\n",
    "    # c is int\n",
    "    coefs = c*[0] + [1]\n",
    "    return np.polynomial.chebyshev.chebval(xs, coefs)\n",
    "def chebX(X, order):\n",
    "    assert (-1 <= X).all() and (X <= 1).all()\n",
    "    \n",
    "    xs = []\n",
    "    for c in range(order):\n",
    "        xs.append(cheb(X, c))\n",
    "    return t.cat(xs, 1)\n",
    "t.manual_seed(0)\n",
    "N = 100\n",
    "X = 2*t.rand(N, 1)-1\n",
    "W_true = t.tensor([[4.]])\n",
    "Y = Bernoulli(logits=X@W_true).sample()\n",
    "\n",
    "def plot(order):\n",
    "    Xe= chebX(X, order)\n",
    "    W = t.zeros((order, 1), requires_grad=True)\n",
    "\n",
    "    for i in range(15000):\n",
    "        L = Bernoulli(logits=Xe@W).log_prob(Y).sum()\n",
    "        if 0==i % 2000:\n",
    "            print(L.item())\n",
    "        dW = t.autograd.grad(outputs=L, inputs=(W,))[0]\n",
    "        W.data += 0.001*dW\n",
    "    \n",
    "    fig, ax = plt.subplots()\n",
    "    ax.set_xlabel(\"$x$\")\n",
    "    ax.set_ylabel(\"probability / $y$\")\n",
    "    ax.scatter(X, Y)\n",
    "    xs = t.linspace(-1, 1, 100)[:, None]\n",
    "    ps_fitted = Bernoulli(logits=chebX(xs, order)@W).probs.detach()\n",
    "    ps_true = Bernoulli(logits=xs@W_true).probs.detach()\n",
    "    ax.plot(xs, ps_fitted, label=\"fitted probability\")\n",
    "    ax.plot(xs, ps_true, label=\"true probability\")\n",
    "    ax.legend()\n",
    "    \n",
    "interact_manual(plot, order=IntSlider(min=2, max=30));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use the same techniques to control overfitting:\n",
    "<ul>\n",
    "    <li> Regularisation </li>\n",
    "    <li> Cross-validation (either on the number of classification errors for test points, or on the test-log-likelihood </li>\n",
    "    <li> Bayesian inference (though its much harder here because the posterior over the weights doesn't have an analytiv form)\n",
    "<ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Simpler, heuristic methods for classification </h2>\n",
    "\n",
    "We've seen a bunch of complex methods, and implementing them is often quite involved.  Is there anything simpler?\n",
    "\n",
    "The answer is yes!\n",
    "\n",
    "<h3> K-nearest neighbour </h3>\n",
    "\n",
    "One approach is to look at the nearby datapoints.  If the nearby points come from one class, then the chances are that our datapoint come from the same class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3055e6fa35084185861ca792a2b18eee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=1, description='K', max=11, min=1, step=2), Button(description='Run Inte…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "iris = datasets.load_iris()\n",
    "_X = t.tensor(iris['data'][50:, [0, 2]]).float()\n",
    "_Y = (t.tensor(iris['target'][50:])-1)\n",
    "\n",
    "t.manual_seed(0)\n",
    "perm = t.randperm(_X.shape[0])\n",
    "X = _X[perm, :]\n",
    "Y = _Y[perm]\n",
    "\n",
    "\n",
    "def plot(K):\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.set_xlabel(\"$x_{\\lambda 0}$\")\n",
    "    ax.set_ylabel(\"$x_{\\lambda 1}$\")\n",
    "    ax.scatter(_X[:, 0], _X[:, 1], c=_Y);\n",
    "\n",
    "    x0range = t.linspace(*ax.get_xlim(), 500)\n",
    "    x1range = t.linspace(*ax.get_ylim(), 500)\n",
    "\n",
    "    x0s, x1s = t.meshgrid(x0range, x1range)\n",
    "    xs = t.stack([x0s, x1s], -1)\n",
    "\n",
    "    X_exp = X[:, None, None, :]\n",
    "    \n",
    "    dist2 = ((X_exp - xs)**2).sum(-1)\n",
    "    elems = (-dist2).topk(K, dim=0).indices\n",
    "    pred = (0.5<Y[elems].float().mean(0)).float()\n",
    "    ax.contour(x0s, x1s, pred)\n",
    "    \n",
    "interact_manual(plot, K=IntSlider(min=1, max=11, step=2));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Cross-validation </h3>\n",
    "\n",
    "Choose $K$ using cross-validation.  Note that leave-one-out cross validation is very suitable here, as we make a prediction for each point, based on its neighbours, in parallel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Weighted-nearest neighbour </h3>\n",
    "\n",
    "This approach is to take a weighted average of nearby datapoints,\n",
    "\n",
    "\\begin{align}\n",
    "  p_{\\lambda, y} &= \\frac{\\sum_{\\lambda'}k(\\x_\\lambda, \\x_{\\lambda'}) \\delta_{y_\\lambda, y_{\\lambda'}}}{\\sum_{\\lambda''} k(\\x_\\lambda, \\x_{\\lambda''})}\n",
    "\\end{align}\n",
    "\n",
    "where we might use a squared-exponential kernel/weights (but many other choices are available),\n",
    "\n",
    "\\begin{align}\n",
    "  k(\\x_\\lambda, \\x_{\\lambda'}) = e^{-(\\x_\\lambda - \\x_{\\lambda'})^2/(2 b)}\n",
    "\\end{align}\n",
    "\n",
    "where $b$ is a bandwidth/lengthscale parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c970ededd62c42ecbad813740db6ad31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=0.15, description='bandwidth', max=1.0, min=0.15, step=0.01), Button(d…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "iris = datasets.load_iris()\n",
    "_X = t.tensor(iris['data'][50:, [0, 2]]).float()\n",
    "_Y = (t.tensor(iris['target'][50:])-1)\n",
    "\n",
    "t.manual_seed(0)\n",
    "perm = t.randperm(_X.shape[0])\n",
    "X = _X[perm, :]\n",
    "Y = _Y[perm]\n",
    "\n",
    "def plot(bandwidth):\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.set_xlabel(\"$x_{\\lambda 0}$\")\n",
    "    ax.set_ylabel(\"$x_{\\lambda 1}$\")\n",
    "    ax.scatter(_X[:, 0], _X[:, 1], c=_Y);\n",
    "\n",
    "    x0range = t.linspace(*ax.get_xlim(), 500)\n",
    "    x1range = t.linspace(*ax.get_ylim(), 500)\n",
    "\n",
    "    x0s, x1s = t.meshgrid(x0range, x1range)\n",
    "    xs = t.stack([x0s, x1s], -1)\n",
    "\n",
    "    X_exp = X[:, None, None, :]\n",
    "\n",
    "    dist2 = ((X_exp - xs)**2).sum(-1)\n",
    "    ws = t.exp(-dist2/(2*bandwidth**2))\n",
    "    pred = (ws*Y_train[:, None, None]).sum(0) / ws.sum(0)\n",
    "\n",
    "    ax.contour(x0s, x1s, 0.5<pred);\n",
    "    \n",
    "interact_manual(plot, bandwidth=FloatSlider(min=0.15, max=1., step=0.01));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Cross-validation </h3>\n",
    "\n",
    "Choose bandwidth using cross-validation.  Note that leave-one-out cross validation is very suitable here, as we make a prediction for each point, based on its neighbours, in parallel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Nearest centroids </h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Using Bayes theorem to do classification </h2>\n",
    "\n",
    "Above, we directly learned parameters that map from the data point, to the class.  Critically, though, we never write down a distribution over data-points, $\\P{x_\\lambda| y_\\lambda}$, so we never have a way to generate something that looks like the data.\n",
    "\n",
    "However, we could do this.  In particular, for each class, we could separately learn $\\P{x_\\lambda| y_\\lambda}$, then use Bayes theorem to give us the probability of a class, conditioned on a data-point,\n",
    "\n",
    "\\begin{align}\n",
    "  \\P{y_\\lambda| \\x_\\lambda} &\\propto \\P{y_\\lambda} \\P{\\x_\\lambda| y_\\lambda}.\n",
    "\\end{align}\n",
    "\n",
    "The simplest example is to take,\n",
    "\n",
    "\\begin{align}\n",
    "  \\P{\\x_\\lambda| y_\\lambda} &= \\N{\\x_\\lambda; \\m_{y_\\lambda}, \\sigma^2 \\I}.\n",
    "\\end{align}\n",
    "\n",
    "Critically, this distribution is easy to fit (in comparison to _clustering_, which we will look at later), because $y_\\lambda$ is known."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/laurence/programs/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:12: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  if sys.path[0] == '':\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8728a2e193774076b53cf8fcce956d64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x7ffe8b1f39e8>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "iris = datasets.load_iris()\n",
    "X = t.tensor(iris['data'][:100, [0, 2]]).float()\n",
    "Y = t.tensor(iris['target'][:100])\n",
    "\n",
    "X0s = X[Y==0, :]\n",
    "X1s = X[Y==1, :]\n",
    "\n",
    "m0 = X0s.mean(0)\n",
    "m1 = X1s.mean(0)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.set_xlabel(\"$x_{\\lambda 0}$\")\n",
    "ax.set_ylabel(\"$x_{\\lambda 1}$\")\n",
    "ax.scatter(X[:, 0], X[:, 1], c=Y)\n",
    "ax.scatter(\n",
    "    np.array([m0[0], m1[0]]), \n",
    "    np.array([m0[1], m1[1]]),\n",
    "    c=np.array([0,1]),\n",
    "    s=300\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cdf0254d0e864e66851517bce00744aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=0, description='y', max=1), Button(description='Run Interact', style=But…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Ns = [\n",
    "    MvNormal(m0, 0.3*t.eye(2)),\n",
    "    MvNormal(m1, 0.3*t.eye(2))\n",
    "]\n",
    "\n",
    "x0range = t.linspace(*ax.get_xlim(), 100)\n",
    "x1range = t.linspace(*ax.get_ylim(), 100)\n",
    "\n",
    "x0s, x1s = t.meshgrid(x0range, x1range)\n",
    "xs = t.stack([x0s, x1s], -1)\n",
    "\n",
    "def prob(y):\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.set_xlabel(\"$x_{\\lambda 0}$\")\n",
    "    ax.set_ylabel(\"$x_{\\lambda 1}$\")\n",
    "    ax.scatter(X[:, 0], X[:, 1], c=Y)\n",
    "    p = Ns[y].log_prob(xs).exp()\n",
    "    ax.contour(x0s, x1s, p)\n",
    "\n",
    "interact_manual(prob, y=IntSlider(min=0, max=1));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To select the class, we use,\n",
    "\n",
    "\\begin{align}\n",
    "  \\P{y_\\lambda| \\x_\\lambda} &\\propto \\P{y_\\lambda} \\P{\\x_\\lambda| y_\\lambda}\n",
    "\\end{align}\n",
    "\n",
    "We take $\\P{y_\\lambda}$ to be a uniform Bernoulli/Categorical, so this distribution is,\n",
    "\n",
    "\\begin{align}\n",
    "  \\log \\P{y_\\lambda| \\x_\\lambda} &= \\log \\P{\\x_\\lambda| y_\\lambda} + \\text{const}\\\\\n",
    "  \\log \\P{y_\\lambda| \\x_\\lambda} &= -\\tfrac{1}{2 \\sigma^2} \\b{\\x_\\lambda - \\m_{y_\\lambda}}^2 + \\text{const}\\\\\n",
    "\\end{align}\n",
    "\n",
    "So finding the $y_\\lambda$ with the maximum posterior probability is equivalent to taking the closest \"centroid\" (i.e. $\\m_{y_\\lambda}$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "820b3bf9947b4c0c9a569aa0cdc79899",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.set_xlabel(\"$x_{\\lambda 0}$\")\n",
    "ax.set_ylabel(\"$x_{\\lambda 1}$\")\n",
    "ax.scatter(X[:, 0], X[:, 1], c=Y)\n",
    "unnorm_p0s = Ns[0].log_prob(xs).exp()\n",
    "unnorm_p1s = Ns[1].log_prob(xs).exp()\n",
    "norm_p1 = unnorm_p1s/(unnorm_p0s + unnorm_p1s)\n",
    "\n",
    "ax.contour(x0s, x1s, norm_p1);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
