{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch as t\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "%matplotlib widget\n",
    "from ipywidgets import FloatSlider, interact, interact_manual"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\newcommand{\\bracket}[3]{\\left#1 #3 \\right#2}\n",
    "\\newcommand{\\b}{\\bracket{(}{)}}\n",
    "\\newcommand{\\Bernoulli}{{\\rm Bernoulli}\\b}\n",
    "\\newcommand{\\x}{\\mathbf{x}}\n",
    "\\newcommand{\\X}{\\mathbf{X}}\n",
    "\\newcommand{\\m}{\\boldsymbol{\\mu}}\n",
    "\\newcommand{\\P}{{\\rm P}\\b}\n",
    "\\newcommand{\\dd}[2][]{\\frac{\\partial #1}{\\partial #2}}\n",
    "\\newcommand{\\S}{\\mathbf{\\Sigma}}\n",
    "\\newcommand{\\Sh}{\\mathbf{\\hat{\\Sigma}}}\n",
    "\\newcommand{\\mh}{\\boldsymbol{\\hat{\\mu}}}\n",
    "\\newcommand{\\N}{\\mathcal{N}\\b}\n",
    "\\newcommand{\\det}{\\bracket{\\lvert}{\\rvert}}\n",
    "\\newcommand{\\sb}{\\bracket{[}{]}}\n",
    "\\newcommand{\\E}{\\mathbb{E}\\sb}\n",
    "\\newcommand{\\Var}{{\\rm Var}\\sb}\n",
    "\\newcommand{\\Cov}{{\\rm Cov}\\sb}\n",
    "\\DeclareMathOperator*{\\argmax}{arg\\,max}\n",
    "\\DeclareMathOperator*{\\argmin}{arg\\,min}\n",
    "\\newcommand{\\ph}{\\hat{p}}\n",
    "\\newcommand{\\at}{\\bracket{.}{\\rvert}}\n",
    "\\newcommand{\\w}{\\mathbf{w}}\n",
    "\\newcommand{\\W}{\\mathbf{W}}\n",
    "\\newcommand{\\W}{\\mathbf{W}}\n",
    "\\newcommand{\\Wh}{\\mathbf{\\hat{W}}}\n",
    "\\newcommand{\\Y}{\\mathbf{Y}}\n",
    "\\newcommand{\\L}{\\mathcal{L}}\n",
    "\\newcommand{\\wh}{\\mathbf{\\hat{w}}}\n",
    "\\newcommand{\\y}{\\mathbf{y}}\n",
    "\\newcommand{\\0}{\\mathbf{0}}\n",
    "\\newcommand{\\I}{\\mathbf{I}}\n",
    "$$\n",
    "\n",
    "<h1> Lecture 1: Supervised learning: classification and regression </h1>\n",
    "\n",
    "First things first:\n",
    "<ul>\n",
    "  <li> I'm experimenting with doing the lectures in Jupyter Notebooks.</li>\n",
    "  <li> Hopefully, it means we can connect theory and code more closely.</li>\n",
    "  <li> I'm going to use the PyTorch distributions library, which is much better than that from Numpy/Scipy.</li>\n",
    "  <li> Don't be scared of PyTorch: the syntax is almost exactly the same as Numpy.</li>\n",
    "  <li> There may be some repetition from previous lectures, but I'll be doing things much more deeply, with more simulations</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In supervised learning, we have a bunch of input, $x$, output, $y$, pairs that are data.\n",
    "\n",
    "The great thing about supervised learning is that the inputs and outputs could be almost anything,\n",
    "```\n",
    "x :: Image,  y :: Int               # Object recognition\n",
    "x :: Image,  y :: Matrix{Int}       # Image segmentation\n",
    "x :: Audio,  y :: Str               # Speech recognition\n",
    "x :; Str,    y :: Audio             # Text-to-speech\n",
    "```\n",
    "The goal is to learn something about the mapping from $x$ to $y$.\n",
    "\n",
    "One approach is to learn a function that maps from $x$ to a guess about the corresponding $y$.  This guess is called $\\hat{y}$,\n",
    "\n",
    "\\begin{align}\n",
    "  f(x) \\rightarrow \\hat{y}\n",
    "\\end{align}\n",
    "\n",
    "To learn $f$, we try to make $y$ from the data as similar as possible to the estimates, $\\hat{y}$.\n",
    "\n",
    "However, to choose this function, we require a measure of \"similiarity\", and it isn't always possible to give one.\n",
    "\n",
    "Instead, we usually write a function that takes an $x$ and returns a distribution over $y$,\n",
    "\n",
    "\\begin{align}\n",
    "  f(x) \\rightarrow \\P{y| x}\n",
    "\\end{align}\n",
    "\n",
    "It is easier to fit such a distribution, because we maximise the probability of the $y$ that we saw in the data.\n",
    "\n",
    "Note: classification and regression are special types of supervised learning.  In classification, the output is, a class-label,\n",
    "```\n",
    "Y = Int     #classification\n",
    "```\n",
    "in regression, the output is one (or many) real values,\n",
    "```\n",
    "Y = Float   #regression\n",
    "```\n",
    "If the output is more complicated (e.g. a string, image or audio), then its neither regression or classification, its just supervised learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Multivariate linear regression </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most useful and instructive supervised learning method is multivariate linear regression.\n",
    "\n",
    "The input for the $\\lambda$th data point is, $\\x$, is a vector and we consider the multi-output case, where the corresponding output, $\\y_\\lambda$ is also a vector.\n",
    "We assume that $\\y_\\lambda$ is Gaussian, conditioned on $\\x$,\n",
    "\n",
    "\\begin{align}\n",
    "  \\P{\\y_{\\lambda}| \\x_\\lambda, \\W} &= \\N{\\y_{\\lambda}; \\x_{\\lambda} \\W^T, \\sigma^2 \\I}\n",
    "\\end{align}\n",
    "\n",
    "Note, here, I'm treating. $\\y_\\lambda$ and $\\x_\\lambda$ as row-vectors, not column vectors, as is standard in maths.  Turns out this allows us to almost directly translate maths to Numpy/PyTorch, so this it really helps for us!\n",
    "\n",
    "If the covariance is diagonal, then we can write the distribution as a Gaussian over each element, $Y_{\\lambda i}$ separately,\n",
    "\n",
    "\\begin{align}\n",
    "  \\P{Y_{\\lambda i}| \\X, \\W} &= \\N{Y_{\\lambda i}; \\sum_j X_{\\lambda j} W_{i j}, \\sigma^2 \\I}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to work with the multi-output case, as it makes the choice of different dimensions/transposes a bit more obvious,\n",
    "\n",
    "\\begin{align}\n",
    "  \\L\\b{\\W} &= \\sum_{\\lambda i} \\log \\P{Y_{\\lambda i}| \\X, \\W}\\\\\n",
    "  \\L\\b{\\W} &= \\sum_{\\lambda i} \\sb{ -\\tfrac{1}{2} \\log 2 \\pi \\sigma^2 - \\tfrac{1}{2 \\sigma^2} \\b{Y_{\\lambda i} - \\sum_j X_{\\lambda j} W_{ij}}^2}.\n",
    "\\end{align}\n",
    "\n",
    "The maximum likelihood estimate, $\\Wh$, is the maximum of the log-likelihood,\n",
    "\n",
    "\\begin{align}\n",
    "  \\Wh &= \\argmax_\\W \\L\\b{\\W}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To compute the maximum, we take the gradient,\n",
    "\n",
    "\\begin{align}\n",
    "  \\dd[\\L\\b{\\W}]{W_{\\alpha \\beta}} &= - \\frac{1}{2 \\sigma^2} \\dd{W_{\\alpha \\beta}} \\sum_{\\lambda i} \\b{Y_{\\lambda i} - \\sum_j X_{\\lambda j} W_{ij}}^2.\n",
    "\\end{align}\n",
    "\n",
    "This looks hard.  But because we've written it in index notation, we can do everything in terms of standard calculus.  We start by applying the chain rule,\n",
    "\n",
    "\\begin{align}\n",
    "  \\dd[\\L\\b{\\W}]{W_{\\alpha \\beta}} &= - \\frac{1}{\\sigma^2} \\sum_{\\lambda i} \\b{Y_{\\lambda i} - \\sum_j X_{\\lambda j} W_{ij}} \\dd{W_{\\alpha \\beta}} \\b{Y_{\\lambda i} - \\sum_j X_{\\lambda j} W_{ij}}.\n",
    "\\end{align}\n",
    "\n",
    "interlude: the Kronecker delta (which looks alot like an identity matrix),\n",
    "\n",
    "\\begin{align}\n",
    "  \\dd[W_{ji}]{W_{\\alpha \\beta}} = \\delta_{i \\alpha} \\delta_{j \\beta}\\\\\n",
    "  \\delta_{ij} = \\begin{cases}\n",
    "    1 & \\text{ if } i = j\\\\\n",
    "    0 & \\text{ if } i \\neq j\n",
    "  \\end{cases}\n",
    "\\end{align}\n",
    "\n",
    "thus,\n",
    "\n",
    "\\begin{align}\n",
    "  \\dd[\\L\\b{\\W}]{W_{\\alpha \\beta}} &= \\frac{1}{\\sigma^2} \\sum_{\\lambda i} \\b{Y_{\\lambda i} - \\sum_j X_{\\lambda j} W_{ij}} \\sum_j X_{\\lambda j} \\delta_{i \\alpha} \\delta_{j \\beta}.\n",
    "\\end{align}\n",
    "\n",
    "rearranging, so the $\\delta$'s are next to the corresponding sum,\n",
    "\n",
    "\\begin{align}\n",
    "  \\dd[\\L\\b{\\W}]{W_{\\alpha \\beta}} &= \\frac{1}{\\sigma^2} \\sum_\\lambda \\underbrace{\\sum_i \\delta_{i \\alpha}}_{\\text{set } i=\\alpha} \\b{Y_{\\lambda i} - \\sum_j X_{\\lambda j} W_{ij}} \\underbrace{\\sum_j \\delta_{j \\beta}}_{\\text{set } j=\\beta} X_{\\lambda j}.\n",
    "\\end{align}\n",
    "\n",
    "the $\\delta_{i \\alpha}$ picks out the $i=\\alpha$ term in the sum, and the $\\delta_{j \\beta}$ picks out the $j=\\beta$ term in the sum,\n",
    "\n",
    "\\begin{align}\n",
    "  \\dd[\\L\\b{\\W}]{W_{\\alpha \\beta}} &= \\frac{1}{\\sigma^2} \\sum_{\\lambda} \\b{Y_{\\lambda \\alpha} - \\sum_j X_{\\lambda j} W_{\\alpha j}} X_{\\lambda \\beta}.\n",
    "\\end{align}\n",
    "\n",
    "Finally, put everything back in vector/matrix notation,\n",
    "\n",
    "\\begin{align}\n",
    "  \\dd[\\L\\b{\\W}]{\\W} &= \\tfrac{1}{\\sigma^2} \\b{\\Y - \\X \\W^T}^T \\X\n",
    "\\end{align}\n",
    "\n",
    "We could just follow the gradient uphill (and that's what we do in deep learning), but this is super-slow.  Instead, there's an answer that is much faster to compute: at the top of the hill, the gradient is zero,\n",
    "\n",
    "\\begin{align}\n",
    "  \\0 &= \\at{\\dd[\\L\\b{\\W}]{\\W}}_{\\W=\\Wh}\\\\ \n",
    "  \\0 &= \\tfrac{1}{\\sigma^2} \\b{\\Y - \\X \\Wh^T}^T \\X \\\\\n",
    "  \\0 &= \\b{\\Y^T - \\Wh \\X^T} \\X\\\\\n",
    "  \\Wh \\X^T \\X &= \\Y^T \\X\n",
    "\\end{align}\n",
    "\n",
    "Note that we can't just solve for $\\Wh$ using the inverse of $\\X^T$ because it is almost never square, so the inverse does not exist!  As, $\\X^T \\X$ is square, we can take its inverse (note that $\\X^T$ isn't square, so we can't take its inverse,)\n",
    "\n",
    "\\begin{align}\n",
    "  \\Wh  &= \\Y^T \\X \\b{\\X^T \\X}^{-1}\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "I've done it this way, because the translation into Numpy/PyTorch is most direct, but if you look things up, you will often find the transposes in different places.\n",
    "\n",
    "Lets write some code!  First, we can directly translate the above expression to give a method for fitting $\\Wh$,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_Wh(X, Y):\n",
    "    return  (Y.T @ X) @ t.inverse(X.T @ X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we generate some 1D \"fake data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3584e475ed3d484face92dfea3037c84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "N     = 100 # number of datapoints\n",
    "D     = 1   # dimension of datapoints\n",
    "sigma = 0.1 # output noise\n",
    "X     = t.randn(N, D)\n",
    "Wtrue = t.ones(1, D)\n",
    "Y     = X @ Wtrue.T + sigma*t.randn(N, 1)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.set_xlabel(\"$x_\\lambda$\")\n",
    "ax.set_ylabel(\"$y_\\lambda$\")\n",
    "ax.scatter(X, Y);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wtrue = tensor([[1.]])\n",
      "Wh    = tensor([[0.9909]])\n"
     ]
    }
   ],
   "source": [
    "Wh = fit_Wh(X, Y)\n",
    "print(f\"Wtrue = {Wtrue}\")\n",
    "print(f\"Wh    = {Wh}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/laurence/programs/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:1: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e00d70d21955434aa3e03953e06e6a25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.set_xlabel(\"$x_\\lambda$\")\n",
    "ax.set_ylabel(\"$y_\\lambda$\")\n",
    "ax.scatter(X, Y, label=\"data\")\n",
    "ax.plot(X, X@Wh.T, 'r', label=\"fitted line\")\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can also generate some 2D \"fake data\","
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/laurence/programs/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:8: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ff2b9a53a6d4ac6a30915ac180b869e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "N     = 100 # number of datapoints\n",
    "D     = 2   # dimension of datapoints\n",
    "sigma = 0.3 # output noise\n",
    "X     = t.randn(N, D)\n",
    "Wtrue = t.ones(1, D)\n",
    "Y     = X @ Wtrue.T + sigma*t.randn(N, 1)\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.set_xlabel(\"$x_{\\lambda, 0}$\")\n",
    "ax.set_ylabel(\"$x_{\\lambda, 1}$\")\n",
    "ax.set_zlabel(\"$y_{\\lambda, 0}$\")\n",
    "ax.scatter(xs=X[:, 0], ys=X[:, 1], zs=Y[:, 0]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wtrue = tensor([[1., 1.]])\n",
      "Wh    = tensor([[0.9781, 0.9921]])\n"
     ]
    }
   ],
   "source": [
    "Wh = fit_Wh(X, Y)\n",
    "print(f\"Wtrue = {Wtrue}\")\n",
    "print(f\"Wh    = {Wh}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/laurence/programs/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:1: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6e61f1c50254fe08e4ec22f05c486cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.set_xlabel(\"$x_{\\lambda, 0}$\")\n",
    "ax.set_ylabel(\"$x_{\\lambda, 1}$\")\n",
    "ax.set_zlabel(\"$y_{\\lambda, 0}$\")\n",
    "ax.scatter(X[:, 0], X[:, 1], Y[:, 0])\n",
    "\n",
    "Xp = t.tensor([\n",
    "    [-4., -4.],\n",
    "    [-4.,  4.],\n",
    "    [ 4., -4.],\n",
    "    [ 4.,  4.]\n",
    "])\n",
    "\n",
    "ax.plot_trisurf(\n",
    "    np.array(Xp[:, 0]), \n",
    "    np.array(Xp[:, 1]), \n",
    "    np.array((Xp @ Wh.T)[:, 0]), \n",
    "    color='r', \n",
    "    alpha=0.3\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But there are issues with linear regression, as formulated thus far.\n",
    "\n",
    "In particular, consider 1D data, generated with a bias,\n",
    "\n",
    "\\begin{align}\n",
    "  \\P{y_\\lambda| \\x_\\lambda, w, b} &= \\N{\\y_\\lambda; x_\\lambda w + b, \\sigma^2}\n",
    "\\end{align}\n",
    "\n",
    "If we fit our old model, without a bias, it doesn't work,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/laurence/programs/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:9: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ad16b7ee1d84299b8b6980cf16b171a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "N     = 100 # number of datapoints\n",
    "D     = 1   # dimension of datapoints\n",
    "sigma = 0.1 # output noise\n",
    "X     = t.rand(N, D)\n",
    "Wtrue = 2*t.ones(1, D)\n",
    "btrue = 3\n",
    "Y     = X @ Wtrue + btrue + sigma*t.randn(N, 1)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.set_xlabel(\"$x_\\lambda$\")\n",
    "ax.set_ylabel(\"$y_\\lambda$\")\n",
    "ax.set_xlim(0, 1)\n",
    "ax.set_ylim(0, 7)\n",
    "ax.scatter(X, Y);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wtrue = tensor([[2.]])\n",
      "Wh    = tensor([[6.6734]])\n"
     ]
    }
   ],
   "source": [
    "Wh = fit_Wh(X, Y)\n",
    "print(f\"Wtrue = {Wtrue}\")\n",
    "print(f\"Wh    = {Wh}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/laurence/programs/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:1: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6cd736a491e24b10ad5cd8ee1e9fff76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.set_xlabel(\"$x_\\lambda$\")\n",
    "ax.set_ylabel(\"$y_\\lambda$\")\n",
    "ax.set_xlim(0, 1)\n",
    "ax.set_ylim(0, 7)\n",
    "ax.scatter(X, Y, label=\"data\")\n",
    "ax.plot(X, X@Wh.T, 'r', label=\"fitted line\")\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What can we do?  Well, we could go through the big derivation above again, incorporating the bias.\n",
    "\n",
    "But this is super-tedious.\n",
    "\n",
    "Instead, note that if we expand the 1D feature vector into a 2D feature vector, with the second feature being just biases,\n",
    "\n",
    "\\begin{align}\n",
    "  \\P{y_\\lambda| \\x_\\lambda, w, b} &= \\N{\\y_\\lambda; \\underbrace{\\begin{pmatrix} x_\\lambda & 1 \\end{pmatrix}}_\\text{expanded feature vector} \\overbrace{\\begin{pmatrix} w \\\\ b \\end{pmatrix}}^\\text{expanded weight vector}, \\sigma^2}.\n",
    "\\end{align}\n",
    "\n",
    "Now, we can just use our original derivation, and implementation!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.7622, 1.0000],\n",
       "        [0.7748, 1.0000],\n",
       "        [0.9505, 1.0000],\n",
       "        [0.0803, 1.0000],\n",
       "        [0.7271, 1.0000],\n",
       "        [0.2640, 1.0000],\n",
       "        [0.8015, 1.0000],\n",
       "        [0.1399, 1.0000],\n",
       "        [0.1650, 1.0000],\n",
       "        [0.4301, 1.0000]])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def add_bias(X):\n",
    "    return t.cat([X, t.ones(X.shape[0], 1)], 1)\n",
    "\n",
    "Xe = add_bias(X)\n",
    "Xe[:10, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wh    = tensor([[1.9719, 3.0323]])\n",
      "Wtrue = tensor([[2.]])\n",
      "btrue = 3\n"
     ]
    }
   ],
   "source": [
    "Wh = fit_Wh(Xe, Y)\n",
    "print(f\"Wh    = {Wh}\")\n",
    "print(f\"Wtrue = {Wtrue}\")\n",
    "print(f\"btrue = {btrue}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/laurence/programs/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:1: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff91b8fdb45d4e808aeb29026b6af55e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.set_xlabel(\"$x_\\lambda$\")\n",
    "ax.set_ylabel(\"$y_\\lambda$\")\n",
    "ax.set_xlim(0, 1)\n",
    "ax.set_ylim(0, 7)\n",
    "ax.scatter(X, Y, label=\"data\")\n",
    "\n",
    "xs = t.tensor([[0.], [1.]])\n",
    "ax.plot(xs, add_bias(xs)@Wh.T, 'r', label=\"fitted line\")\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That was really cool!\n",
    "\n",
    "It turns out that this idea can be taken *much* further.\n",
    "\n",
    "In particular, instead of just incorporating a constant feature, we can incorporate arbitrary, nonlinear *functions* of the original input data.\n",
    "\n",
    "For instance,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/laurence/programs/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:10: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "acf3b02ce7a249599086ea347c06a2d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "N     = 100 # number of datapoints\n",
    "D     = 1   # dimension of datapoints\n",
    "sigma = 0.5 # output noise\n",
    "X     = t.randn(N, D)\n",
    "qtrue = 2  # quadratic term\n",
    "ltrue = -1 # linear term\n",
    "btrue = 1  # bias\n",
    "Y     = qtrue*X**2 + ltrue*X + btrue + sigma*t.randn(N, 1)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.set_xlabel(\"$x_\\lambda$\")\n",
    "ax.set_ylabel(\"$y_\\lambda$\")\n",
    "ax.scatter(X, Y);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.3204, -0.5660,  1.0000],\n",
       "        [ 0.0230, -0.1518,  1.0000],\n",
       "        [ 6.8478,  2.6168,  1.0000],\n",
       "        [ 0.9868, -0.9934,  1.0000],\n",
       "        [ 0.9389,  0.9690,  1.0000],\n",
       "        [ 0.0296, -0.1721,  1.0000],\n",
       "        [ 6.4896, -2.5475,  1.0000],\n",
       "        [ 0.0751, -0.2740,  1.0000],\n",
       "        [ 0.0469,  0.2166,  1.0000],\n",
       "        [ 0.0121,  0.1102,  1.0000]])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def quad(X):\n",
    "    return t.cat([X**2, X, t.ones(X.shape[0], 1)], 1)\n",
    "\n",
    "Xe = quad(X)\n",
    "Xe[:10, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wh    = tensor([[ 2.0368, -1.0608,  0.9671]])\n",
      "qtrue = 2\n",
      "ltrue = -1\n",
      "btrue = 1\n"
     ]
    }
   ],
   "source": [
    "Wh = fit_Wh(Xe, Y)\n",
    "print(f\"Wh    = {Wh}\")\n",
    "print(f\"qtrue = {qtrue}\")\n",
    "print(f\"ltrue = {ltrue}\")\n",
    "print(f\"btrue = {btrue}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/laurence/programs/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:1: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae8e0326ec7245b5be7dca9e26c79681",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.set_xlabel(\"$x_\\lambda$\")\n",
    "ax.set_ylabel(\"$y_\\lambda$\")\n",
    "\n",
    "ax.scatter(X, Y, label=\"data\")\n",
    "\n",
    "xs = t.linspace(-4, 4, 100)[:, None]\n",
    "ax.plot(xs, quad(xs)@Wh.T, 'r', label=\"fitted line\")\n",
    "ax.legend();"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
