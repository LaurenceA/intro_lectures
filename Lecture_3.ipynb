{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch as t\n",
    "from torch.distributions import MultivariateNormal as MvNormal\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "%matplotlib widget\n",
    "from ipywidgets import FloatSlider, IntSlider, interact, interact_manual"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\newcommand{\\bracket}[3]{\\left#1 #3 \\right#2}\n",
    "\\newcommand{\\b}{\\bracket{(}{)}}\n",
    "\\newcommand{\\Bernoulli}{{\\rm Bernoulli}\\b}\n",
    "\\newcommand{\\x}{\\mathbf{x}}\n",
    "\\newcommand{\\X}{\\mathbf{X}}\n",
    "\\newcommand{\\m}{\\boldsymbol{\\mu}}\n",
    "\\newcommand{\\P}{{\\rm P}\\b}\n",
    "\\newcommand{\\dd}[2][]{\\frac{\\partial #1}{\\partial #2}}\n",
    "\\newcommand{\\S}{\\mathbf{\\Sigma}}\n",
    "\\newcommand{\\Sh}{\\mathbf{\\hat{\\Sigma}}}\n",
    "\\newcommand{\\mh}{\\boldsymbol{\\hat{\\mu}}}\n",
    "\\newcommand{\\N}{\\mathcal{N}\\b}\n",
    "\\newcommand{\\det}{\\bracket{\\lvert}{\\rvert}}\n",
    "\\newcommand{\\sb}{\\bracket{[}{]}}\n",
    "\\newcommand{\\E}{\\mathbb{E}\\sb}\n",
    "\\newcommand{\\Var}{{\\rm Var}\\sb}\n",
    "\\newcommand{\\Cov}{{\\rm Cov}\\sb}\n",
    "\\DeclareMathOperator*{\\argmax}{arg\\,max}\n",
    "\\DeclareMathOperator*{\\argmin}{arg\\,min}\n",
    "\\newcommand{\\ph}{\\hat{p}}\n",
    "\\newcommand{\\at}{\\bracket{.}{\\rvert}}\n",
    "\\newcommand{\\w}{\\mathbf{w}}\n",
    "\\newcommand{\\W}{\\mathbf{W}}\n",
    "\\newcommand{\\W}{\\mathbf{W}}\n",
    "\\newcommand{\\Wh}{\\mathbf{\\hat{W}}}\n",
    "\\newcommand{\\Y}{\\mathbf{Y}}\n",
    "\\newcommand{\\L}{\\mathcal{L}}\n",
    "\\newcommand{\\wh}{\\mathbf{\\hat{w}}}\n",
    "\\newcommand{\\y}{\\mathbf{y}}\n",
    "\\newcommand{\\0}{\\mathbf{0}}\n",
    "\\newcommand{\\I}{\\mathbf{I}}\n",
    "\\newcommand{\\La}{\\mathbf{\\Lambda}}\n",
    "\\newcommand{\\S}{\\mathbf{\\Sigma}}\n",
    "\\newcommand{\\Sprior}{\\S_\\text{prior}}\n",
    "\\newcommand{\\Spost}{\\S_\\text{post}}\n",
    "\\newcommand{\\mprior}{\\m_\\text{prior}}\n",
    "\\newcommand{\\mpost}{\\m_\\text{post}}\n",
    "\\newcommand{\\Xt}{\\tilde{\\X}}\n",
    "\\newcommand{\\yt}{\\tilde{\\y}}\n",
    "$$\n",
    "\n",
    "<h1> Lecture 3: Bayesian regression </h1>\n",
    "\n",
    "Previously, we had written down a model for $\\Y$ conditioned on $\\W$,\n",
    "\n",
    "\\begin{align}\n",
    "  \\P{Y_{\\lambda i}| \\X, \\W} &= \\N{Y_{\\lambda i}; \\sum_j X_{\\lambda j} W_{j i}, \\sigma^2 \\I}\n",
    "\\end{align}\n",
    "\n",
    "This means that given inputs, $\\X$, and weights, $\\W$, (and $\\sigma$), we can simulate what the outputs, $\\Y$ might look like.\n",
    "\n",
    "Now, we're going to write down a model for the weights,\n",
    "\n",
    "\\begin{align}\n",
    "  \\P{\\W} &= \\prod_i \\N{\\w_i; \\0, \\Sprior}\n",
    "\\end{align}\n",
    "\n",
    "where $\\w_i$ is the $i$th row of $W_{ij}$, `W[i, :]` (i.e. all the weights for the $i$th output).\n",
    "\n",
    "<h2> Getting insight into the model by sampling functions </h2>\n",
    "Samples of $\\W$ now correspond to functions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c95ac196f4442148517be3bb03d89c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=1, description='order', max=20, min=1), FloatSlider(value=1.0, descripti…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def cheb(xs, c):\n",
    "    # c is int\n",
    "    coefs = c*[0] + [1]\n",
    "    return np.polynomial.chebyshev.chebval(xs, coefs)\n",
    "\n",
    "def chebX(X, order):\n",
    "    assert (-1 <= X).all() and (X <= 1).all()    \n",
    "    xs = []\n",
    "    for c in range(order):\n",
    "        xs.append(cheb(X, c))\n",
    "    return t.cat(xs, 1)\n",
    "\n",
    "N     = 100  # number of datapoints\n",
    "D     = 1   # dimension of datapoints\n",
    "\n",
    "def plot_function(order, Sigma_prior_scale):\n",
    "    xs = t.linspace(-1, 1, 100)[:, None]\n",
    "    Sigma_prior = Sigma_prior_scale * t.eye(order)\n",
    "    \n",
    "    fig, ax = plt.subplots()\n",
    "    ax.set_xlabel(\"$x$\")\n",
    "    ax.set_ylabel(\"$y$\")\n",
    "    ax.set_xlim(-1, 1)\n",
    "    ax.set_ylim(-4, 4)\n",
    "    \n",
    "    for i in range(10):\n",
    "        W = MvNormal(t.zeros(1, order), Sigma_prior).sample()\n",
    "        Y = chebX(xs, order) @ W.T\n",
    "        ax.plot(xs, Y)\n",
    "    \n",
    "interact_manual(plot_function, order=IntSlider(min=1, max=20), Sigma_prior_scale=FloatSlider(min=0.01, max=1, value=1, step=0.01));\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4655fe13453546a4a36f8b460138215d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=1, description='order', max=20, min=1), FloatSlider(value=1.0, descripti…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_function(order, Sigma_prior_scale, decay):\n",
    "    xs = t.linspace(-1, 1, 100)[:, None]\n",
    "    Sigma_prior = Sigma_prior_scale * t.eye(order) * t.exp(-decay * t.arange(order))\n",
    "    \n",
    "    print(Sigma_prior)\n",
    "    \n",
    "    fig, ax = plt.subplots()\n",
    "    ax.set_xlabel(\"$x$\")\n",
    "    ax.set_ylabel(\"$y$\")\n",
    "    ax.set_xlim(-1, 1)\n",
    "    ax.set_ylim(-4, 4)\n",
    "    \n",
    "    for i in range(10):\n",
    "        W = MvNormal(t.zeros(1, order), Sigma_prior).sample()\n",
    "        Y = chebX(xs, order) @ W.T\n",
    "        ax.plot(xs, Y)\n",
    "    \n",
    "interact_manual(plot_function, \n",
    "                order=IntSlider(min=1, max=20), \n",
    "                Sigma_prior_scale=FloatSlider(min=0.01, max=1, value=1, step=0.01),\n",
    "                decay=FloatSlider(min=0., max=2., value=1, step=0.01)\n",
    "               );\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is useful, because we can set/initialize the parameters, $\\S$, by generating functions that look similar to the data (e.g. similar range on $y$, similar wiggliness).  \n",
    "\n",
    "This generative approach gives us a sensible setting for our parameters, and is thus much better than regularisation parameters, which could be anything.\n",
    "\n",
    "We will see later how to fine-tune these values using data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Computing Bayesian posteriors over the weights </h2>\n",
    "\n",
    "Once we have defined a sensible function class, we can perform inference to find the posterior distribution over the weights.\n",
    "\n",
    "This gives us not just a \"best-guess\", output for any given test-point, but a full distribution.\n",
    "\n",
    "We consider a single-output, for which,\n",
    "```\n",
    "y = Y[:, 0]\n",
    "```\n",
    "\n",
    "\\begin{align}\n",
    "  y_\\lambda &= Y_{\\lambda, 0}\\\\\n",
    "\\end{align}\n",
    "\n",
    "we consider both $\\y$ and $\\w$ as column vectors.  As multivariate normal in PyTorch takes row-vectors, we write,\n",
    "\n",
    "\\begin{align}\n",
    "  \\P{\\w} &= \\N{\\w; \\0, \\Sprior}\\\\\n",
    "  \\P{\\y| \\w, \\X} &= \\N{\\y; \\X \\w^T, \\sigma^2 \\I}\n",
    "\\end{align}\n",
    "\n",
    "where this is one big multivariate normal distribution over all datapoints jointly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying Bayes theorem,\n",
    "\n",
    "\\begin{align}\n",
    "  \\log \\P{\\w| \\y, \\X} &= \\log \\P{\\y| \\w, \\X} + \\log \\P{\\w} + \\text{const}\n",
    "\\end{align}\n",
    "\n",
    "where,\n",
    "\n",
    "\\begin{align}\n",
    "  \\log \\P{\\y| \\w, \\X} &= - \\tfrac{1}{2\\sigma^2} \\b{\\y - \\X \\w^T}^T \\b{\\y - \\X \\w^T} + \\text{const}\\\\\n",
    "  \\log \\P{\\y| \\w, \\X} &= - \\tfrac{1}{2\\sigma^2} \\b{\\y^T \\y - 2 \\w \\X^T \\y + \\w \\X^T \\X \\w^T} + \\text{const}\\\\\n",
    "  \\log \\P{\\y| \\w, \\X} &= - \\tfrac{1}{2\\sigma^2} \\b{- 2 \\w \\X^T \\y + \\w \\X^T \\X \\w^T} + \\text{const}\\\\\n",
    "  \\log \\P{\\w} &= - \\tfrac{1}{2} \\w \\Sprior^{-1} \\w^T + \\text{const}\n",
    "\\end{align}\n",
    "\n",
    "thus, the posterior is,\n",
    "\n",
    "\\begin{align}\n",
    "  \\log \\P{\\w| \\y, \\X} &=  - \\tfrac{1}{2\\sigma^2} \\w \\X^T \\X \\w^T - \\tfrac{1}{2} \\w \\Sprior^{-1} \\w^T + \\tfrac{1}{\\sigma^2} \\w \\X^T \\y + \\text{const}\n",
    "\\end{align}\n",
    "\n",
    "Note that the log-posterior is a quadratic function of $\\w$.  Thus, the posterior is multivariate normal.\n",
    "\n",
    "While we could just rearrange the above expression, it turns out to be easier to name the posterior mean, $\\mprior$, and the posterior covariance, $\\Spost$,\n",
    "\n",
    "\\begin{align}\n",
    "  \\log \\P{\\w| \\y, \\X} &= \\N{\\w; \\mpost, \\Spost}\\\\\n",
    "  \\log \\P{\\w| \\y, \\X} &= -\\tfrac{1}{2} \\b{\\w - \\mpost} \\Spost^{-1} \\b{\\w - \\mpost}^T + \\text{const}\\\\\n",
    "  \\log \\P{\\w| \\y, \\X} &= -\\tfrac{1}{2} \\w \\Spost^{-1} \\w^T + \\w \\Sprior^{-1} \\mpost^T + \\text{const}\n",
    "\\end{align}\n",
    "\n",
    "Now, the two forms for the log-posterior must be equal,\n",
    "\n",
    "\\begin{align}\n",
    "-\\tfrac{1}{2} \\w \\Spost^{-1} \\w^T + \\w \\Spost^{-1} \\mpost^T + \\text{const} &= - \\tfrac{1}{2\\sigma^2} \\w \\X^T \\X \\w^T - \\tfrac{1}{2} \\w \\Sprior^{-1} \\w^T + \\tfrac{1}{\\sigma^2} \\w \\X^T \\y + \\text{const}.\n",
    "\\end{align}\n",
    "\n",
    "The quadratic and linear terms must be separately equal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The quadratic terms allow us to identify $\\Spost$,\n",
    "\n",
    "\\begin{align}\n",
    "  -\\tfrac{1}{2} \\w \\Spost^{-1} \\w^T &= - \\tfrac{1}{2\\sigma^2} \\w \\X^T \\X \\w^T - \\tfrac{1}{2} \\w \\Sprior^{-1} \\w^T\\\\\n",
    "  \\w \\Spost^{-1} \\w^T &= \\tfrac{1}{\\sigma^2} \\w \\X^T \\X \\w^T + \\w \\Sprior^{-1} \\w^T\\\\\n",
    "  \\Spost^{-1} &= \\tfrac{1}{\\sigma^2} \\X^T \\X + \\Sprior^{-1}\\\\ \n",
    "  \\Spost^{-1} &= \\tfrac{1}{\\sigma^2} \\b{\\X^T \\X + \\sigma^2 \\Sprior^{-1}}\\\\ \n",
    "  \\Spost &= \\sigma^2 \\b{\\X^T \\X + \\sigma^2 \\Sprior^{-1}}^{-1}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The linear terms allow us to identify $\\mprior$,\n",
    "\n",
    "\\begin{align}\n",
    "  \\w \\Spost^{-1} \\mpost^T &= \\tfrac{1}{\\sigma^2} \\w \\X^T \\y\\\\\n",
    "  \\Spost^{-1} \\mpost^T &= \\tfrac{1}{\\sigma^2} \\X^T \\y\\\\\n",
    "  \\mpost^T &= \\tfrac{1}{\\sigma^2} \\Spost \\X^T \\y\\\\\n",
    "  \\mpost^T &= \\b{\\X^T \\X + \\sigma^2 \\Sprior^{-1}}^{-1}\\X^T \\y\\\\\n",
    "  \\mpost &= \\y^T \\X \\b{\\X^T \\X + \\sigma^2 \\Sprior^{-1}}^{-1}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: this is exactly the same expression as the one we had for regularised linear regression in the previous lecture, with\n",
    "\n",
    "\\begin{align}\n",
    "  \\La &= \\Sprior^{-1}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We did all that work.  If it just gave us regularised maximum-likelihood, why did we even bother?  Three reasons:\n",
    "\n",
    "<ul>\n",
    "  <li> We can set sensible values for $\\Sprior$ and $\\sigma$ by looking at samples from the generative model </li>\n",
    "  <li> We can plot functions corresponding to weights sampled from the posterior </li>\n",
    "  <li> We can compute a posterior distribution over test outputs, taking into account uncertainty due to noise and uncertainty due to limited training data. </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Sampling the posterior over functions </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c65bbb385544c1388b1739b78f46842",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Button(description='Run Interact', style=ButtonStyle()), Output()), _dom_classes=('widge…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def posterior(X, Y, sigma, Sprior):\n",
    "    s2 = sigma**2\n",
    "    Spost = s2 * t.inverse(X.T@X + s2*t.inverse(Sprior))\n",
    "    mpost= 1/s2 * (Y.T @ X) @ Spost\n",
    "    return MvNormal(mpost, Spost)\n",
    "\n",
    "N     = 20  # number of datapoints\n",
    "D     = 1   # dimension of datapoints\n",
    "sigma = 0.1 # output noise\n",
    "rand1 = (1+t.rand(N//2, 1))/3\n",
    "rand2 = -(1+t.rand(N//2, 1))/3\n",
    "X     = t.cat([rand1, rand2], 0)\n",
    "Wtrue = t.tensor([[0.]])\n",
    "Y     = chebX(X, 1) @ Wtrue + sigma*t.randn(N, 1)\n",
    "\n",
    "\n",
    "def plot():\n",
    "    xs = t.linspace(-1, 1, 100)[:, None]\n",
    "    \n",
    "    fig, ax = plt.subplots()\n",
    "    ax.set_xlabel(\"$x$\")\n",
    "    ax.set_ylabel(\"$y$\")\n",
    "    ax.set_xlim(-1, 1)\n",
    "    ax.set_ylim(-4, 4)\n",
    "    \n",
    "    \n",
    "    for i in range(10):\n",
    "        Wsample = posterior(chebX(X, 10), Y, sigma, t.eye(10)).sample()\n",
    "        ys = chebX(xs, 10) @ Wsample.T\n",
    "        ax.plot(xs, ys, alpha=0.5)\n",
    "        \n",
    "    ax.scatter(X, Y)\n",
    "        \n",
    "interact_manual(plot);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/laurence/programs/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:20: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d8932d7ff444b3e9b3d8d1544949713",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.2764],\n",
       "        [ 0.1650],\n",
       "        [-0.6031],\n",
       "        [ 0.3631],\n",
       "        [-0.1232],\n",
       "        [ 0.8674],\n",
       "        [ 0.3980],\n",
       "        [ 1.0162],\n",
       "        [-0.2576],\n",
       "        [-0.0652]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Computing the posterior over a test point </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above, we plotted the functions corresponding to samples of the weights.\n",
    "\n",
    "But if we have a single test point, this isn't necessarily super-helpful.\n",
    "\n",
    "Instead, we want the posterior mean and variance for that test point.\n",
    "\n",
    "We could get that by sampling many functions, but that's super-slow.\n",
    "\n",
    "Instead,\n",
    "\n",
    "\\begin{align}\n",
    "  \\P{\\yt| \\Xt, \\X, \\y} &= \\int \\mathbf{dW} \\; \\P{\\yt| \\Xt, \\w} \\P{\\w| \\X, \\y}\\\\\n",
    "\\end{align}\n",
    "\n",
    "where,\n",
    "\n",
    "\\begin{align}\n",
    "  \\P{\\yt| \\Xt, \\w} &= \\N{\\yt; \\Xt \\w^T, \\sigma^2 \\I}\\\\\n",
    "  \\P{\\w| \\X, \\y} &= \\N{\\w; \\mpost, \\Spost}\n",
    "\\end{align}\n",
    "\n",
    "Instead of doing the integral, which is a pain, we can rewrite these expressions as random variables,\n",
    "\n",
    "\\begin{align}\n",
    "  \\yt &= \\Xt \\w^T + \\sigma \\boldsymbol{\\xi}\\\\\n",
    "\\end{align}\n",
    "\n",
    "where $\\P{\\boldsymbol{\\xi}} = \\N{\\boldsymbol{\\xi}; \\0, \\I}$.\n",
    "\n",
    "The expected value of $\\yt$ is therefore,\n",
    "\n",
    "\\begin{align}\n",
    "  \\E{\\yt} &= \\E{\\Xt \\w^T + \\sigma \\boldsymbol{\\xi}}\\\\\n",
    "  \\E{\\yt} &= \\Xt \\E{\\w^T}\\\\\n",
    "  \\E{\\yt} &= \\Xt \\mpost^T\n",
    "\\end{align}\n",
    "\n",
    "And the variance of $\\yt$ is,\n",
    "\n",
    "\\begin{align}\n",
    "  \\Cov{\\yt} &= \\Cov{\\Xt \\w^T} + \\sigma^2 \\I\\\\\n",
    "  \\Cov{\\yt} &= \\E{\\Xt \\b{\\w - \\mpost}^T \\b{\\w-\\mpost} \\Xt^T} + \\sigma^2 \\I\\\\\n",
    "  \\Cov{\\yt} &= \\Xt \\E{\\b{\\w - \\mpost}^T \\b{\\w-\\mpost}} \\Xt^T + \\sigma^2 \\I \\\\\n",
    "  \\Cov{\\yt} &= \\Xt \\Spost \\Xt^T + \\sigma^2 \\I\n",
    "\\end{align}\n",
    "\n",
    "Thus,\n",
    "\n",
    "\\begin{align}\n",
    "  \\P{\\yt| \\Xt, \\X, \\y} &= \\N{\\yt^T; \\, \\Xt \\mpost^T, \\, \\Xt \\Spost \\Xt^T + \\sigma^2 \\I}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/laurence/programs/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:3: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34f870920eb044a9a887e51f215ddcd2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "xs = t.linspace(-1, 1, 100)[:, None]\n",
    "    \n",
    "fig, ax = plt.subplots()\n",
    "ax.set_xlabel(\"$x$\")\n",
    "ax.set_ylabel(\"$y$\")\n",
    "ax.set_xlim(-1, 1)\n",
    "ax.set_ylim(-4, 4)\n",
    "    \n",
    "PW = posterior(chebX(X, 10), Y, sigma, t.eye(10))\n",
    "mean = chebX(xs, 10) @ PW.mean.T\n",
    "cov = chebX(xs, 10) @ PW.covariance_matrix[0, :, :] @ chebX(xs, 10).T + sigma**2 * t.eye(xs.shape[0])\n",
    "std = cov.diag()[:, None].sqrt()\n",
    "\n",
    "\n",
    "\n",
    "ax.fill_between(xs[:, 0], (mean - 2*std)[:, 0], (mean + 2*std)[:, 0], alpha=0.3, label=\"mean $\\pm$ 2 std\")\n",
    "ax.plot(xs, mean, label=\"mean\")\n",
    "ax.scatter(X, Y, label=\"data\")\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> How do we set the \"hyperparameters\" (e.g. $\\sigma$), based on data? </h2>\n",
    "\n",
    "We compute the \"marginal likelihood\",\n",
    "\n",
    "\\begin{align}\n",
    "  \\P{\\y| \\X} &= \\int \\mathbf{dW} \\; \\P{\\y| \\w, \\X} \\P{\\w}\n",
    "\\end{align}\n",
    "\n",
    "where,\n",
    "\n",
    "\\begin{align}\n",
    "  \\P{\\y| \\X, \\w} &= \\N{\\y; \\X \\w^T, \\sigma^2 \\I}\\\\\n",
    "  \\P{\\w} &= \\N{\\w; \\0, \\Sprior}\n",
    "\\end{align}\n",
    "\n",
    "We can use a similar method as above (except that now things are much easier).\n",
    "\n",
    "\\begin{align}\n",
    "  \\y &= \\X \\w^T + \\sigma \\boldsymbol{\\xi}\\\\\n",
    "\\end{align}\n",
    "\n",
    "where $\\P{\\boldsymbol{\\xi}} = \\N{\\boldsymbol{\\xi}; \\0, \\I}$.\n",
    "\n",
    "The expected value of $\\y$ is therefore, $\\0$,\n",
    "\n",
    "\\begin{align}\n",
    "  \\E{\\y} &= \\E{\\X \\w^T + \\sigma \\boldsymbol{\\xi}}\\\\\n",
    "  \\E{\\y} &= \\X \\E{\\w^T}\\\\\n",
    "  \\E{\\y} &= \\0\n",
    "\\end{align}\n",
    "\n",
    "And the variance of $\\yt$ is,\n",
    "\n",
    "\\begin{align}\n",
    "  \\Cov{\\y^T} &= \\Cov{\\w \\X^T} + \\sigma^2 \\I\\\\\n",
    "  \\Cov{\\y^T} &= \\E{\\X \\w^T \\w \\X^T} + \\sigma^2 \\I\\\\\n",
    "  \\Cov{\\y^T} &= \\X \\E{\\w^T \\w} \\X^T + \\sigma^2 \\I \\\\\n",
    "  \\Cov{\\y^T} &= \\X \\Sprior \\X^T + \\sigma^2 \\I\n",
    "\\end{align}\n",
    "\n",
    "Thus,\n",
    "\n",
    "\\begin{align}\n",
    "  \\P{\\y| \\X} &= \\N{\\y^T; \\, \\0, \\, \\X \\Sprior \\X^T + \\sigma^2 \\I}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/laurence/programs/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:21: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d26cec83bedf4261a663a205708946f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "N     = 10  # number of datapoints\n",
    "D     = 1   # dimension of datapoints\n",
    "sigma = 0.5 # output noise\n",
    "t.manual_seed(0)\n",
    "rand  = t.rand(N, 1)\n",
    "X     = 2*rand - 1\n",
    "Wtrue = t.tensor([[0.2, 0.5]])\n",
    "Y     = chebX(X2, 2) @ Wtrue.T + sigma*t.randn(N, 1)\n",
    "\n",
    "def marginal_likelihood(X, Y, sigma, order, Sigma_prior_scale):\n",
    "    N = X.shape[0]\n",
    "    Xe = chebX(X, order)\n",
    "    Sprior = Sigma_prior_scale * t.eye(order)\n",
    "    dist = MvNormal(t.zeros(N), Xe @ Sprior @ Xe.T + sigma**2*t.eye(N))\n",
    "    return dist.log_prob(Y.T)\n",
    "\n",
    "log_10_scales = t.linspace(-2.5, 2.5, 100)\n",
    "scales = 10**log_10_scales\n",
    "orders = t.arange(1, 10)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.set_xlabel(\"Sigma prior scale\")\n",
    "ax.set_ylabel(\"marginal_likelihood\")\n",
    "ax.set_xscale(\"log\")\n",
    "ax.set_ylim(-15, -7)\n",
    "\n",
    "for order in range(1, 5):\n",
    "    mls = t.tensor([marginal_likelihood(X, Y, sigma, order, scale) for scale in scales])\n",
    "    ax.plot(scales, mls, label=f\"order={order}\")\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interesting fact: the Bayesian model evidence can tell the difference between different functions, even when they're all well-regularised!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
